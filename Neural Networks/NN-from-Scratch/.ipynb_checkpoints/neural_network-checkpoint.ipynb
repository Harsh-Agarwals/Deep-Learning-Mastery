{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "468d4cc9-6978-4c8f-aecb-4c7afe772c99",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "**This notebook will go through the basics of Modular programming (setting up classes, calling functions from other files, using classes, etc) which will lay the foundation for our own mini python deep-learning library.**\n",
    "\n",
    "- Also, I will make the basic building block of a Neural Neural, slowly increasing it's complexity to a Deep Neural Network with various optimization, processing, regularization and other techniques.\n",
    "- Running the model on different datasets\n",
    "- Devising our own way to analyse results through graphs and visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3514c2b-1bd8-4aca-9448-9972e215db47",
   "metadata": {},
   "source": [
    "## Basics of Neural Network\n",
    "\n",
    "What comes to your mind when you hear the term Neural? I am sure it reminds you of something related to the Human brain. That is actually the place this term Neural Network came from. Our brain is actually a **complex mesh network of \"neurons\"** which processes the incoming information to it and passes it to the next neuron untill it reaches the receptor. This simple process makes our brain control the body functions, including thought, emotion, response, sense, etc.\n",
    "\n",
    "- Here is a simple image of our brain, showing different sections responsible for response to different senses.\n",
    "  <table>\n",
    "      <tr>\n",
    "          <td><img src=\"Images/Senses.png\" style=\"width: 400px\"></td>\n",
    "          <td><img src=\"Images/neuron.png\" style=\"width: 400px\"></td>\n",
    "      </tr>\n",
    "  </table>\n",
    "  \n",
    "- And an image of a neuron which receives information from the dendrites at the one end, processes the information inside and passing it finally to another neuron(whose dendrites are attached to the end of this neuron) through axon terminal.\n",
    "\n",
    "**_Neural Network shares a similar story!_**\n",
    "\n",
    "Neural Network lays the foundation of the modern AI, from your _Google translate_ to your phone's _voice assistant_, from your mail _spam filter_ to the keyboard next words _auto suggestion_, from the _self-driving cars_ to the _ChatGPT_, from _image generation_ softwares(like Midjourney, Dalle) to your Netflix _Recommender system_, and much more...\n",
    "\n",
    "Just like the way brain wires itself to process lots of information, thereby producing a useful output which stimulates responses based on the sensory receptor(smell, hearing, vision, etc) it sourced to, Neural Network too **computes hundreds of thousands of calculation**, processes the information it was given and outputs a useful result for the information. And just how our brain processes information related to hearing, vision, language understanding, logics, etc, Neural Networks strange can process similar information using hundreds of neurons, doing large number of mathematical calculations.\n",
    "\n",
    "- _**Here is a simple Neural Network for predicting hand written digits, enough to spark curiousity in you.**_ Credits: 3blue1brown on youtube.\n",
    "\n",
    "<img src=\"Images/nn.gif\" style=\"margin: 2rem auto\">\n",
    "\n",
    "_In this notebook, we will be going through the basic working of Neural Networks, mathematical intuition behind its working, will build Neural Network of our own, will add lots of complexity to our basic network to make it process large dataset producing better results and finally we will study different variations of Neural Networks for different tasks. So, let's get started!_\n",
    "\n",
    "### So, here is a simple definition of Neural Network!\n",
    "\n",
    "A neural network consist of neurons which are simply a **mathematical function** taking some input, doing some calculations and outputting the value of the function. A NN can have _few neurons to hundreds of thousands of neuron_. Together, they make up a very strong model that works similar to the brain and have the capacity to process any kind of information(text, voice, languages, tabular, logical, etc), given the correct _NN architecture_.\n",
    "\n",
    "**Universal function Approximator:** A Neural Network is popularly known as the **Black Box**, meaning something that can process and approximate anything you put inside the box. So, when we send to the NN a simple data table with X as random numbers and y mapping X to it its square without specifying the function, the NN will figure out the squared function mapping. Similarly, it can figure out any kind of data mapping, approximating for any kind of function, hence abbreviated as _the Universal function approximator_.\n",
    "\n",
    "#### Mathemical intuition\n",
    "\n",
    "_Here is the simple picture to understand an artificial neuron:_\n",
    "\n",
    "<img src=\"Images/artificial_neuron.png\" style=\"margin: 2rem auto; width: 400px\">\n",
    "\n",
    "_Here, the neuron takes inputs(some weights are associated with each input), process the information inside mathematically, and outputs the final value._\n",
    "\n",
    "And here is an image of a simple Neural Network, with each node acting as a neuron, recieving weighted inputs from all the nodes of the previous layer and providing it's weighted output to all the nodes of the subsequent layer.\n",
    "\n",
    "<img src=\"Images/simple_nn.png\" style=\"margin: 2rem auto; width: 200px\">\n",
    "\n",
    "_Here, the input layer consists of input neurons(3 here) taking input data, and output layer consists of output neurons(3 outputs). This Neural Network also has a **hidden layer** with its own neurons(4) making the network more complex and processing the data better (as now more complex calculation are associated with the network, making it unleashing more properties associated with the data)._\n",
    "\n",
    "Here is a simple example, let's say we have an input data consisting of housing prices with data properties like number of bedrooms, number of bathrooms, total squared area, etc. So the hidden layers will figure out the _complex properties associated_ with the input layer, like more prices if area is more, more price if number of bedrooms is more, etc using some complex mathematical function like (for example): $$node\\_1 = \\sqrt{w1*number\\_bedrooms + w2*number\\_bathrooms}$$\n",
    "Each node will perform some complex calculations w.r.t its inputs, associating neuron with some particular hidden properties. These hidden properties then plays a very crucial role in understanding the data, modelling it to learn the **X->y mapping**.\n",
    "\n",
    "#### _Now, lets look at the working of a Neural Network_\n",
    "\n",
    "A neuron, as we discussed earlier, takes weighted inputs from all its precedent layer, wrap it in some function to give output. _And here is how the whole network works_:\n",
    "\n",
    "- First the architechture of the NN is decided (which includes number of hidden layers and number of nodes in each hidden layers)\n",
    "- Since each node of a particular layer will connect to each nodes of the next layer, **weights** of each of the connection is randomly assigned to neurons of each layer, and **bias** is also assigned to each layer nodes.\n",
    "- Calculation from the input layer to the output layer are performed for each layer and its nodes. This process is called **Forward Propagation**.\n",
    "- Final output prediction is generated.\n",
    "- The above process is repeated for all training examples **(in batch, mini-batches or stochastically(one at a time))**, and at the end we define a **Cost function** which measure the collective loss of all the example/s in the batch. There are different cost functions associated with different tasks, like _log loss for classification, squared error for regression_, etc.\n",
    "- Once the cost function is calculated, we _update all the weights and biases_ for every layer in the **backward propagation** step(just like forward propagation, but backwards). This process is the optimization step which aims to _**update and optimize weight and bias**_ to the value that, in the forward pass, produces the value closest to our real value, making correct prediction. There are many **optimization algorithm**, **Gradient Descent** being the most famous and used one.\n",
    "- _**Since it will take many forward and backward passes to reach the optimized value of weights and biases, the above processes(forward propagation, cost updation and backward propagation) are performed in many iterations, eventually reaching the minima.**_\n",
    "\n",
    "\n",
    "##### In this notebook, I aims to discuss all the processes in detail, from scratch, associated with the working, optimization and analysis of Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594eda9-87e3-4853-b358-6f54a6635db5",
   "metadata": {},
   "source": [
    "### Artificial Neuron\n",
    "\n",
    "_Here is an in-depth view of what's happening inside a neuron:_\n",
    "\n",
    "<img src=\"Images/neuron_mathematical.png\" style=\"margin: 2rem auto; height: 250px\">\n",
    "\n",
    "As we can see here, values from all the nodes of the previous layer, with their associated weights and a bias, are passed as inputs to the neuron. There are **two functions(linear function and activation function)** inside the neuron that computes the input one after the other, outputting the final result. _Let's look at both the functions in detail:_\n",
    "\n",
    "**Linear function:** It is a simple function that computes the **linear combination** of all the inputs, adding the bias to it at the end. This is useful as it _tells how much focus our this particular neuron(which gets understanding a particular property over time) has to give to the output of each of the previous neurons_. Bias term adjust the output of the linear function toward positive or negative side of the activation function, improving model's flexibility and boosting it's performance (don't worry, we will study this part later). _So, the equation for a particular data(j) instance having n columns is:_\n",
    "$$\\hat{y_j} = \\sum_{i=1}^{n} x_j^{(i)}w_j^{(i)} + b_j$$\n",
    "_And, equation for m data instances having n columns each is:_\n",
    "$$\\hat{y} = \\sum_{i=1}^{m} w^TX^{(i)} + b$$\n",
    "where, $$X \\in \\mathbb{R}^{(n, m)}$$ \n",
    "$$w \\in \\mathbb{R}^{(n, 1)}$$ \n",
    "$$ b\\in \\mathbb{R}$$ \n",
    "$$\\hat{y} \\in \\mathbb{R}^{(1, m)}$$ \n",
    "for m examples. We will later learn how to initialize these weights and bias, and how to set up a particular NN architecture. _Now we pass this output $\\hat{y}$ to the activation function._\n",
    "\n",
    "**Activation function:**\n",
    "\n",
    "Why are activation function important? This might come to your mind that why do we need an another function if we had already worked out on one function. And what is the role of this activation function in this step? So, here we go-\n",
    "\n",
    "Let's consider a Neural Network with 10 hidden layers, and for each layer we just calculate the linear combination of inputs. We will have $\\hat{y}^1 = w^1X + b, \\hat{y}^2 = w^2\\hat{y}^1 + b = w^1w^2X + b, ...$ So, we will have $\\hat{y}^{10} = w^1w^2...w^{10}X + b$ which is equivalent to $wX + b$. **So, neural network here failed to learn the complex non-linear relationships.**\n",
    "\n",
    "So, we see these hidden layers is not affecting our final results much. It is equivalent to having one hidden layer, and training it well to produce linear result. **_So, this tells us that we need an another function, that does not just produce a linear function, rather some other function, that makes deeper layers output useful to the final output_**. _Here comes the concept of **Non-liner activations**. These are the activation functions that we pass our linear functions output to, and the output makes sure that every layer gives a non linear output, helpful in finding different properties associated with the data in the middle layers._\n",
    "\n",
    "Another reason why we use non-linear activation function in each neuron is because of the backpropagation step. _In the backpropagation step, we calculate the derivative of outputs of each neuron, and this derivative helps in the optimization step using Gradient Descent_. So we chose such non-linear activation function that have steeper derivatives, that can **optimize gradient descent faster**. Also, since the outputs (specially in classification tasks) are discrete, we chose the final function as the one closer to our output range. Usually we chose **Sigmoid function** or **Softmax function** for the final layer of the NN with discrete output, and **linearReLU** for regression problems. In the middle layers, we generally use **ReLU, leaky ReLU, tanh or sigmoid function**.\n",
    "\n",
    "_**These functions provides non-linearity to our outputs, which helps in learning the complex non-linear relationships, and also helps with faster optimization during backpropagation.** Here are some activation functions for reference:_ These can be referenced <a href=\"activation.py\">here</a>.\n",
    "\n",
    "Sigmoid: $$z = \\frac{1}{1+e^{-x}}$$\n",
    "Softmax for class j: $$z_j = \\frac{e^{x_j}}{\\sum_{i=1}^{n} e^{x_i}}$$\n",
    "Tanh: $$z = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "Relu: $$z = Max(0, x)$$\n",
    "Leaky Relu: $$z = Max(0.01x, x)$$\n",
    "\n",
    "Now, since we know the building blocks of a Neural Network, let's dive deeper to understand _Forward Propagation._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39bdd7-7110-41f8-8687-03fddfcb40a6",
   "metadata": {},
   "source": [
    "## Forward Propagation in Neural Network\n",
    "\n",
    "Forward propagation is the initial step in training a Neural Network where **we calculate the value of each neurons of each layer** of the NN in the forward pass. After passing through all the layers, we _finally output our prediction_ for that particular example. This is the prediction generated from the particular sets of weights and bias, and as our model matures(through optimization and backpropagation), we will shift towards a particular sets of parameters that makes our predictions really close to its actual value.\n",
    "\n",
    "_So, in this step, given a set of weights and bias, we are just forward propagating our calculations from input layer, via hidden layer to the output layer, outputting our final predictions from this set of W&B. Here is a visual map of the process (Credit: https://yogayu.github.io/DeepLearningCourse)_\n",
    "\n",
    "<img src=\"Images/fwd.gif\" style=\"width: 400px; margin: auto\">\n",
    "\n",
    "Here we can see how at each layer, we calculate the nodes values, which in turn are used to calculated the values of the nodes of the next layer. Finally in the sequence, we calculate the output node values. **This final output value will go to our cost function to determine the deviation in predictions, which accordingly will set further steps of backpropagation.** _But before going into cost function, let's discuss the very first step, which is **Weights initialization**._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad32e4-97aa-4152-9648-69f4218104e5",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "_Long awaited, we are going to discuss the full Neural Network architecture here. We will understand layers, their shapes, weight and biases, their shape, output, and much more._\n",
    "\n",
    "Let's take an example of the following Neural Network:\n",
    "\n",
    "<img src=\"Images/nn_architecture.png\" style=\"margin: 1.5rem auto; width: 300px\">\n",
    "\n",
    "This is a Neural Network with:\n",
    "- Input layer having 4 neurons\n",
    "- 2 hidden layers having 3 and 4 neurons respectively\n",
    "- Output layer with 2 neurons\n",
    "\n",
    "Notations:\n",
    "- $L$: Total number of layers(excluding input layer)\n",
    "- $l$: Current layer into consideration\n",
    "- $w^{[l]}, b^{[l]}$: weights and bias associated with layer $l$\n",
    "- $n$: Number of features in input\n",
    "- $m$: Number of training examples\n",
    "- $y$: actual values\n",
    "- $\\hat{y}$: predicted values\n",
    "- $X$: input vector\n",
    "- $z^{[l]}$: linear output of layer $l$\n",
    "- $a^{[l]}$: final output of layer $l$, after passing it through the activation function\n",
    "- $J$: Cost function\n",
    "\n",
    "**Here, $L=3$**, so we have layer 1($a^{[1]}$ with 3 neurons), layer 2($a^{[2]}$ with 4 neurons) and layer 3($a^{[3]}$ with output layer with 2 neurons). We call input layer as layer 0($a^{[0]}$).\n",
    "\n",
    "Since X is the input of size $m$ with $n$ features, shape of $X$ is $\\mathbb{R}^{(n, m)}$. This $X$ vector will be passed as the input layer to the NN as $a^{[0]}$ having shape $\\mathbb{R}^{(n, m)}$. Size of layer $1(z^{[1]}, a^{[1]})$ will be $\\mathbb{R}^{(3, m)}$, layer $2(z^{[2]}, a^{[2]})$ $\\mathbb{R}^{(4, m)}$ and size of layer $3(z^{[3]}, a^{[3]})$ is $\\mathbb{R}^{(2, m)}$.\n",
    "\n",
    "**Now for each layer(except 0), we will have set of weights and biases.** So, we will have $w^{[1]}, b^{[1]}, w^{[2]}, b^{[2]}, w^{[3]}$ and $b^{[3]}$\n",
    "\n",
    "For any layer $l$, weight of that layer is of the shape $(layer\\_size\\_[l]$, $layer\\_size\\_{[l+1]})$. Example, here:\n",
    "- **size of $w^{[1]}$** = $\\mathbb{R}^{(3, 4)}$ because size of layer 1 is 3 and size of layer 0 is 4 so 3x4\n",
    "- **size of $w^{[2]}$** = $\\mathbb{R}^{(4, 3)}$ because size of layer 2 is 4 and size of layer 1 is 3 so 4x3\n",
    "- **size of $w^{[3]}$** = $\\mathbb{R}^{(2, 4)}$ because size of layer 3 is 2 and size of layer 2 is 4 so 2x4\n",
    "\n",
    "And, shape of bias is $(1, layer\\_size\\_[l])$, So:\n",
    "- **size of $b^{[1]}$** = $\\mathbb{R}^{(1, 3)}$,\n",
    "- **size of $b^{[2]}$** = $\\mathbb{R}^{(1, 4)}$,\n",
    "- **size of $b^{[3]}$** = $\\mathbb{R}^{(1, 2)}$\n",
    "\n",
    "**Calculation of each layer:**\n",
    "- **Step 1:** $z^{[l]} = {w^{[l]}}^{T}a^{[l-1]} + b^{[l]}$ => output size: $\\mathbb{R}^{(3, m)}$ for $z^{[1]}$\n",
    "- **Step 2:** $a^{[l]} = g(z^{[l]})$, where $g$ is the activation function\n",
    "- **Final layer:** $\\hat{y} = g(z^{[L]})$ where $z^{[L]} = {w^{[L]}}^Ta^{[L-1]} + b^{[L]}$. Size of $\\hat{y}$ is same as the size of $y$ which is $\\mathbb{R}^{(1, m)}$\n",
    "\n",
    "**For better understanding, consider the following diagram:**\n",
    "\n",
    "<img src=\"Images/nn_simplified.png\" style=\"width: 600px; margin: 1rem auto\">\n",
    "\n",
    "_Now, after calculating $\\hat{y}$, we calculate Cost function $J$ for $\\hat{y}$ with respect to $y$. Cost function is a single value output comparing $\\hat{y}$ and $y$. Following this, we do backpropagation._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79c109-d15e-4680-bb9d-b27b5aec3e60",
   "metadata": {},
   "source": [
    "## Neural Networks weights initialization\n",
    "\n",
    "Weights initialization is the process of _assigning weights to the layers of neural network_. A good weights initialization method is very important, as it can **faster optimize** the model towards the minima, **improving its performance**. Also having a good weights helps us to **deal with Exploding and vanishing gradients**. Since we don't have any idea about weights initially, we assign weights randomly, and based on some research, there are few ways to assign weights that helps the most. This also helps **prevent overfitting**.\n",
    "\n",
    "_**Some methods of weights initialization are:**_ <a href=\"weights_init.py\">Reference</a>\n",
    "\n",
    "1. **Zeroes initialization:** Assigns all the weights to 0. This does not help because $w := w - \\alpha \\frac{\\nabla J}{\\nabla w}$ _[this is a gradient descent equation, we will learn it soon]_ will make **assign similar weights to all the neurons** in a layer, hence not helping much, it is actually similar to a linear model.\n",
    "2. **Random initialization:** Assigning all the weights randomly prevents symmetry like in zeros initialization. It helps to learn parameters better.\n",
    "3. **He initialization:** random_initialization*$\\sqrt{\\frac{2}{size^{[l-1]}}}$, where $l$ is the present layer\n",
    "4. **Xavier/Glorot initialization:** random_initialization*$\\sqrt{\\frac{1}{size^{[l-1]}}}$\n",
    "5. **Other methods:** random_initialization*$\\sqrt{\\frac{1}{size^{[l-1]} + size^{[l]}}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2228ac09-9635-497f-a218-cf1d567a8e11",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "**Why Cost function?**\n",
    "\n",
    "After going throgh the forward propagation and predicting our result, we now need to _look into the correctness of our model result_. Since the model will be immature at the beginning with bad sets of W&B, we should expect bad predictions at the beginning, and with more iterations and model training, with improving set of W&B, we will observe **improvement in the predictions over time**. _But how should we measure the correctness of the model result? **Loss function** and **Cost function** are the answers to this question, which we will study here in detail._\n",
    "\n",
    "**Choosing a Cost function**\n",
    "\n",
    "_There are different types of mathematical functions that can be used to compare actual and predicted values, but there are few considerations to choose a cost function, some of which are:_\n",
    "\n",
    "- Cost function depends on the type of NN task(Classification, Regression, GAN, etc)\n",
    "- It must be such that it penalises the bad predictions and does least harm to the correct predictions\n",
    "- It must be a good evaluation for the NN task\n",
    "\n",
    "**Based on different tasks, we have different types of Cost functions like:** (Referenced <a href=\"loss.py\">here</a>)\n",
    "\n",
    "- **Regression**: _mean square error loss function, mean absolute error cost function_\n",
    "\n",
    "_**Mean Square error Cost function** for m examples:_\n",
    "$$J = -\\frac{1}{m} \\sum_{i=1}^{m} (y^{(i)} - \\hat{y}^{(i)})^2$$\n",
    "This square in this function **penalizes bad predictions more** than the correct predictions, making it really good for many regression tasks.\n",
    "\n",
    "_**Mean Absolute error Cost function** for m examples:_\n",
    "$$J = \\frac{1}{m} \\sum_{i=1}^{m} |y^{(i)} - \\hat{y}^{(i)}|$$\n",
    "This absolute function **is very robust to outliers** and gives really good results when our datasets has many outliers.\n",
    "\n",
    "- **Binary Classification**: _binary cross entropy cost function (also called as log loss function)_\n",
    "\n",
    "_**Binary Cross entropy function** for m examples:_\n",
    "$$J = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)}.log(\\hat{y}^{(i)}) + (1 - y^{(i)}).log(1 - \\hat{y}^{(i)})]$$\n",
    "This works very well for almost all binary class problems, and **approximates really well** for bad predictions.\n",
    "\n",
    "- **Multiclass Classification**: _categorical cross entropy(predicts probability of every class) and sparse categorical cross entropy(predicts index of the predicted calss) cost function_\n",
    "$$J = -\\sum_{i=1}^{m} y^{(i)}.log(\\hat{y}^{(i)})$$\n",
    "- other application cost function: like **triple loss(for Neural style transfer), Wasserstein loss(For GANs), BLEU score(for NLP tasks)**, etc\n",
    "\n",
    "_**So, depending on the task, we will choose our Cost function.**_\n",
    "\n",
    "### Now, every part of the forward propagation is done. It's time for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26708513-f3e3-40e6-874f-52c8096cc51f",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Backpropagation is a very crucial step in training a Neural Neural. This step involves running the optimization algorithm, which optimizes the weights and bias of th NN, leading to the optimal solution. _Main optimizing algorithms used in this process are:_\n",
    "- Gradient Descent\n",
    "- Mini-batch Gradient Descent\n",
    "- Stochastic Gradient Descent\n",
    "\n",
    "The optimization algorithms goes backward(from the last layer to the initial layer), hence backpropagation, and **computes the derivative** of forward propagation outputs of each layer, **updating weights and biases** of each layer, one at a time. Every iteration starts with computing derivative of the cost function, then moving backwards.\n",
    "\n",
    "This process is _repeated untill we reaches the minima(global minima for GD, and local minima for mini-batch and SGC) and the cost value gets very small and stabilises_ over multiple iterations.\n",
    "\n",
    "**_Here is how the optimization process runs:_**\n",
    "\n",
    "    repeat untill convergence:\n",
    "        for i in num_iterations:\n",
    "            for i in batch(could be 1:batch, many:mini-batch, or m:SGD):\n",
    "                update w\n",
    "                update b\n",
    "\n",
    "**_And here is how the whole training process is done:_**\n",
    "\n",
    "    for each iteration:\n",
    "        forward propagation (compute hidden layer activation and y_hat)\n",
    "        compute cost\n",
    "        backpropagation\n",
    "        update weights and bias\n",
    "    finally after the optimization, predict with the final set of W&B\n",
    "\n",
    "_Here is a reference to the training of the NN:_\n",
    "\n",
    "<img src=\"Images/full.gif\" style=\"margin: auto; width: 550px\">\n",
    "\n",
    "### Parameters updates process\n",
    "\n",
    "**weights updation** $$w := w - \\alpha \\frac{\\delta J}{\\delta w}$$ where $\\frac{\\delta J}{\\delta w}$ is the _derivative of Cost with respect to weight w_\n",
    "\n",
    "**bias updation** $$b := b - \\alpha \\frac{\\delta J}{\\delta b}$$ where $\\frac{\\delta J}{\\delta b}$ is the _derivative of Cost with respect to bias b_\n",
    "\n",
    "### Backpropagation in-depth intuition\n",
    "\n",
    "The above section highlighted the overview of the working of the backpropagation algorithm. In this section, let's see an in-depth intuition of working of the backpropagation algorithm and different optimizing algorithms. As discussed earlier, backpropagation starts with the last step of the forward propagation, which is calculating the cost function. So, in backpropagation, at every layer, we calculate the derivative of activations associated with that layer, later using this derivative to update the weights and bias. But before these, we calculate the derivative of the cost function. \n",
    "\n",
    "**Here is a reason why we calculate derivatives:**\n",
    "\n",
    "Since our parameters(W&B) updations required $\\frac{\\delta{J}}{\\delta{w}}$ and $\\frac{\\delta{J}}{\\delta{b}}$ for every $w$ and $b$, we are need to calculate the derivative for each layer in the reverse manner.\n",
    "\n",
    "_**For the last layer($a^{[L]}$ and $z^{[L]}$), with parameters $w^{[L]}$ and $b^{[L]}$, updating $w^{[L]}$ and $b^{[L]}$ requires calculating $\\frac{\\delta{J}}{\\delta{w^{[L]}}}$ and $\\frac{\\delta{J}}{\\delta{b^{[L]}}}$, which is calculated using:**_\n",
    "\n",
    "$$\\frac{\\delta{J}}{\\delta{a^{[L]}}} since\\ J = f(y, a^{[L]})$$\n",
    "\n",
    "Now, as we know $$a^{[L]} = g(z^{[L]}) = sigmoid({w^{[L]}}^Ta^{[L-1]} + b^{[L]}),$$ to calculate $\\frac{\\delta{J}}{\\delta{w^{[L]}}}$ and $\\frac{\\delta{J}}{\\delta{b^{[L]}}}$, \n",
    "\n",
    "we requires calculating $\\frac{\\delta{a^{[L]}}}{\\delta{z^{[L]}}}$ to get \n",
    "$$\\frac{\\delta{J}}{\\delta{z^{[L]}}} (\\frac{\\delta{J}}{\\delta{a^{[L]}}}.\\frac{\\delta{a^{[L]}}}{\\delta{z^{[L]}}}), $$\n",
    "\n",
    "then calculating $\\frac{\\delta{z^{[L]}}}{\\delta{w^{[L]}}}$ and $\\frac{\\delta{z^{[L]}}}{\\delta{b^{[L]}}}$ to get $$\\frac{\\delta{J}}{\\delta{w^{[L]}}} (\\frac{\\delta{J}}{\\delta{z^{[L]}}}.\\frac{\\delta{J}}{\\delta{w^{[L]}}})$$ and \n",
    "$$\\frac{\\delta{J}}{\\delta{b^{[L]}}} (\\frac{\\delta{J}}{\\delta{z^{[L]}}}.\\frac{\\delta{J}}{\\delta{b^{[L]}}})$$.\n",
    "\n",
    "_**Now we use these derivatives to update our parameters of the last layer. Similarly, we will move to the next layer(from the back) to calculate the derivatives to that layer to update it's parameters. This is repeated till we update parameters of the first layer,** \"then again ready for **forward propagation with these new sets of parameters.**\"_\n",
    "\n",
    "_Here is a simple gif for better understanding:_\n",
    "\n",
    "<img src=\"Images/bkp.gif\" style=\"margin: 0 auto; width: 400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8fa55-d1ec-4be9-88ca-047e43b98ea6",
   "metadata": {},
   "source": [
    "## Now, lets do the whole Backpropagation maths:\n",
    "\n",
    "### Task: Regression\n",
    "\n",
    "For regression task, our cost function is: $J(y, \\hat{y}) = -\\frac{1}{m} \\sum_{i=1}^{m} (y-\\hat{y})^2 = -\\frac{1}{m} \\sum_{i=1}^{m} (y-a^{[3]})^2$\n",
    "\n",
    "**Layer L($l$=3)**\n",
    "\n",
    "Let's calculate $\\frac{\\delta{J}}{\\delta{a^{[3]}}}$, which is $\\frac{2}{m} (y - a^{[3]})$, equivialent to $y - a^{[3]}\\ i.e.$ output layer will not have any activation function, and is simply a linear output.\n",
    "\n",
    "Now, since this is regression task, $a^{[3]} = z^{[3]}$\n",
    "\n",
    "<!-- So, $\\frac{\\delta{a^{[3]}}}{\\delta{z^{[3]}}} = \\begin{dcases} 1,& \\text{if } x\\gt 0\\\\0,& \\text{otherwise}\\end{dcases}$  -->\n",
    "\n",
    "or $\\frac{\\delta{a^{[3]}}}{\\delta{z^{[3]}}} = 1$\n",
    "\n",
    "Now, we have $\\frac{\\delta{J}}{\\delta{z^{[3]}}} = y - a^{[3]}$, where $z^{[3]} = {w^{[3]}}^Ta^{[2]} + b^{[3]}$\n",
    "\n",
    "So, now we have $\\frac{\\delta{z^{[3]}}}{\\delta{w^{[3]}}} = a^{[2]}$, and $\\frac{\\delta{z^{[3]}}}{\\delta{b^{[3]}}} = 1$, leaving us with\n",
    "\n",
    "$$\\frac{\\delta{J}}{\\delta{w^{[3]}}} = (y-a^{[3]}).a^{[2]}$$, and\n",
    "$$\\frac{\\delta{J}}{\\delta{b^{[3]}}} = (y-a^{[3]})$$\n",
    "\n",
    "Now, we will update our weights as: $w^{[3]} := w^{[3]} - \\alpha\\frac{\\delta{J}}{\\delta{w^{[3]}}}$ and $b^{[3]} := b^{[3]} - \\alpha\\frac{\\delta{J}}{\\delta{b^{[3]}}}$\n",
    "\n",
    "**Layer L - 1 ($l$=2)**\n",
    "\n",
    "For the second layer, we have the following:\n",
    "\n",
    "$w^{[2]}, b^{[2]}, a^{[1]}\\ =>\\ z^{[2]}\\ =>\\ sigmoid\\ =>\\ a^{[2]}$\n",
    "\n",
    "$$\\frac{\\delta{J}}{\\delta{w^{[2]}}}=\\frac{\\delta{J}}{\\delta{a^{[2]}}}.\\frac{\\delta{a^{[2]}}}{\\delta{z^{[2]}}}.\\frac{\\delta{z^{[2]}}}{\\delta{w^{[2]}}}$$\n",
    "\n",
    "$\\frac{\\delta{J}}{\\delta{a^{[2]}}}=\\frac{\\delta{J}}{\\delta{z^{[3]}}}.\\frac{\\delta{z^{[3]}}}{\\delta{a^{[2]}}}$, where $\\frac{\\delta{J}}{\\delta{z^{[3]}}}=y-a^{[3]}$ and $\\frac{\\delta{z^{[3]}}}{\\delta{a^{[2]}}}=w^{[3]}$\n",
    "\n",
    "So, $\\frac{\\delta{J}}{\\delta{a^{[2]}}}=(y-a^{[3]}).w^{[3]}$, and $a^{[2]}=sigmoid(z^{[2]})$. So, $\\frac{\\delta{a^{[2]}}}{\\delta{z^{[2]}}}=sigmoid(z^{[2]}).(1-sigmoid(z^{[2]})$\n",
    "\n",
    "And $z^{[2]}={w^{[2]}}^Ta^{[1]}+b^{[2]}$, so, $\\frac{\\delta{z^{[2]}}}{\\delta{w^{[2]}}}=a^{[1]}$ and $\\frac{\\delta{z^{[2]}}}{\\delta{b^{[2]}}}=1$\n",
    "\n",
    "Together, we have: $$\\frac{\\delta{J}}{\\delta{w^{[2]}}}=(y-a^{[3]}).w^{[3]}.\\{sigmoid(z^{[2]}).(1-sigmoid(z^{[2]})\\}.a^{[1]}$$\n",
    "and, $$\\frac{\\delta{J}}{\\delta{b^{[2]}}}=(y-a^{[3]}).w^{[3]}.\\{sigmoid(z^{[2]}).(1-sigmoid(z^{[2]})\\}$$\n",
    "\n",
    "And, we now update our weights and bias as: $w^{[2]} := w^{[2]} - \\alpha\\frac{\\delta{J}}{\\delta{w^{[2]}}}$ and $b^{[2]} := b^{[2]} - \\alpha\\frac{\\delta{J}}{\\delta{b^{[2]}}}$\n",
    "\n",
    "_**Similarly, we will do calculation for the first layer of NN to update $w^{[1]}$ and $b^{[1]}$.**_\n",
    "\n",
    "### Task: Classification\n",
    "\n",
    "For Binary classification task, our cost function is: $J(y, \\hat{y}) = -\\frac{1}{m} \\sum_{i=1}^{m} y^{(i)}.log(\\hat{y}^{(i)}) + (1-y^{(i)}).log(1-\\hat{y}^{(i)})$\n",
    "\n",
    "Similar to what we have done in Regression, we do in Classification, except with two difference. Can you figure it out?\n",
    "\n",
    "In the regression task, we had output layer $a^{[L]}$ same as $z^{[L]}$, which is not the case for **Binary or multilabel Classification**, also we have different cost function associated with different types of classification(Binary and multiclass). For Binary, we have $a^{[L]} = sigmoid(z^{[L]})$ and for Multiclass, we have $a^{[L]} = softmax(z^{[L]})$. \n",
    "\n",
    "So now, $$\\frac{\\delta{J}}{\\delta{a^{[3]}}}=\\sum_{i=1}^{m} -\\frac{y^{(i)}}{a^{[3]}}+\\frac{1-y^{(i)}}{1-{a^{[3]}}^{(i)}}$$\n",
    "and $$\\frac{\\delta{a^{[3]}}}{\\delta{z^{[3]}}}=sigmoid(z^{[3]}).(1-sigmoid(z^{[3]}))$$\n",
    "_And accordingly, our other layer values will change during backpropagation._\n",
    "\n",
    "So, backpropagation at these steps are different from that of regression, and since all other early layers are same, _backpropagation there is similar to that of Regression_.\n",
    "\n",
    "**This is a very complex calculation, and as we go deeper, the complexity keeps increasing. So, to not to worry about these computations, modern Deep Learning frameworks like _Tensorflow_ and _Pytorch_ takes care of this step itself, and we don't need to do any physical calculations for these.**\n",
    "\n",
    "#### And following this, we again do the forward propagation, cost computation and backpropagation. And this step is repeated till the convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4b945",
   "metadata": {},
   "source": [
    "## Optimization Algorithms\n",
    "\n",
    "**There are many optimization algorithms that works on implementing these all steps together, thereby updating our parameters. Some of these algorithms are:**\n",
    "\n",
    "- **Gradient Descent** (applied on the whole training dataset together in one batch)\n",
    "- **Mini-batch Gradient Descent** (applied on the whole training dataset, in mini-batches, having multiple training examples)\n",
    "- **Stochastic Gradient Descent** (applied on the whole training dataset, one at a time)\n",
    "\n",
    "<img src=\"Images/optimizer.gif\" style=\"margin: 0 auto; width: 400px\">\n",
    "\n",
    "_**Batch Gradient Descent converges the parameters to their global minima, whereas mini-batch and SGD converges to the local minima. Also, there is a smooth convergence in case of Batch GD, and zig-zag rough convergence in case of others. For Batch GD, Cost $J$ strictly decreases with iteration, whereas for other, this may not be the case.**_\n",
    "\n",
    "_Here is the comparison:_\n",
    "\n",
    "<img src=\"Images/gd.png\" style=\"margin: 0 auto; width: 500px\">\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"Images/batch-contour.png\" style=\"margin: 0 auto; width: 300px\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"Images/mini-batch-contour.png\" style=\"margin: 0 auto; width: 500px\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"Images/contour.jpg\" style=\"margin: 0 auto; width: 500px\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "_The above shown contours are for Batch GD and Mini-batch GD_\n",
    "\n",
    "### Working of Optimization Algorithm\n",
    "\n",
    "Optimization algorithm takes one/more training example at a time, passes it through our Neural Network, computes the Cost, and updates the weights using backpropagation. They continue doing this for **all the training examples for the given number of iterations**, untill we reaches a convergence point from where there is _very slow progress that this point can be assumed a minima_. The parameters at this stage are the final parameters of the NN, and our NN is trained at this point with these sets of parameters(W&B).\n",
    "\n",
    "_One pass through all the training examples is called an **Epoch.**_ For training a NN, we specifies the number of epoch we want our model to train during optimization.\n",
    "\n",
    "**Here is how the optimization algorithm runs:**\n",
    "\n",
    "    repeat untill convergence:\n",
    "        for i in num_iterations:\n",
    "            for i in batch(could be 1:batch, many:mini-batch, or m:SGD):\n",
    "                forward propagation (compute hidden layer activation and y_hat)\n",
    "                compute cost\n",
    "                backpropagation\n",
    "                update w\n",
    "                update b\n",
    "                \n",
    "**And this is how we update the parameters:**\n",
    "\n",
    "**weights updation** $$w := w - \\alpha \\frac{\\delta J}{\\delta w}$$ where $\\frac{\\delta J}{\\delta w}$ is the _derivative of Cost with respect to weight w_\n",
    "\n",
    "**bias updation** $$b := b - \\alpha \\frac{\\delta J}{\\delta b}$$ where $\\frac{\\delta J}{\\delta b}$ is the _derivative of Cost with respect to bias b_\n",
    "\n",
    "for each layer of the NN.\n",
    "\n",
    "**$\\alpha$ here is the hyperparameter: Means a parameter whose value we will define manually using trail and errors.**\n",
    "\n",
    "_**Since $\\alpha$ affects the rate at which we will update our parameters, we need to be very cautious with chosing $\\alpha$.**_\n",
    "\n",
    "- If it's value is too high, the derivative gets updated very fast, missing the ideal sets of W&B, shooting the values very quickly.\n",
    "- If it's value is too low, it will converge very slowly, which can take too longer to train our network.\n",
    "- So, it's value is needed to be in perfect range.\n",
    "\n",
    "_Usually, we choose alpha in ranges from 0.001 to 1, like 0.001, 0.003, 0.006, 0.01, and so on._\n",
    "\n",
    "### Evaluation of the model\n",
    "\n",
    "After all the training and optimization process how do we make sure that our model is trained perfectly and optimized to the minima that gives a good result? There are various parameters to check the sancity of our model and algorithms. Some of these are:\n",
    "\n",
    "- Checking Loss v/s Number of iteration graph\n",
    "- Checking R2 score, mean absolute error, etc in case of Regression task on the test set\n",
    "- Checking Precision, Recall, f1-score, AUC-ROC curve, etc on Classification tasks on the test set\n",
    "\n",
    "_For optimization, we check the graph of Loss with respect to the iterations._\n",
    "\n",
    "<img src=\"Images/loss.png\" style=\"margin: 0 auto; width: 400px\">\n",
    "\n",
    "**If there is an overall decrease in cost of the training process with iteration, our algorithm is optimizing well.**\n",
    "\n",
    "#### Till when do we optimize?\n",
    "\n",
    "We optimize till the point our algorithm reaches the desired minima giving us a good result. **Usually, we train to a point where Cost function $J$ stops improving with respect to iterations.** Training beyond that point where there is very slighter improvements makes no difference, and is only the wastage of computational resources and time in training our network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04ac03",
   "metadata": {},
   "source": [
    "#### So, in this notebook we discussed the basics of neural networks. Now, there are many problems associated with this implementation, some of which includes -\n",
    "\n",
    "- Skewed Input features (Solution: Normalization, Batch Normalization)\n",
    "- Vanishing and Exploding gradients (Solution: Random initialization of weights, gradient clipping, batch norm, etc)\n",
    "- Slow and skewed optimization (Solution: momentum, RMSprop, Adam, learning rate decay)\n",
    "- Handling bias and variance (Solution: Dropout and L1/L2 regularization, early stopping)\n",
    "\n",
    "**We will discuss all these issues and solution to the different problems associated with the Neural Networks in the next notebook. Also, this notebook contains our Python & Numpy implementation of the NN, to look into DAX, Tensorflow and Pytorch implementation, please visit the respective folder in the main repo.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
