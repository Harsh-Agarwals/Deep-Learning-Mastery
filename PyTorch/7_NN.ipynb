{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNeuralNet(\n",
       "  (fully_connected1): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (fully_connected2): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fully_connected1 = nn.Linear(10, 5)\n",
    "        self.fully_connected2 = nn.Linear(5, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fully_connected1(x))\n",
    "        x = self.fully_connected2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNeuralNet().to(device=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomDataset at 0x200f0c2d430>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "    \n",
    "data, labels = [\"a\", \"b\"], [1, 2]\n",
    "dataset = CustomDataset(data, labels)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x200f0c35bb0>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset=dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('b',)\n",
      "tensor([2])\n"
     ]
    }
   ],
   "source": [
    "for X, y in dataloader:\n",
    "    print(X)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000, 2), (3000,))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_data(samples=1000, input_size=2, num_classes=2):\n",
    "    X = np.random.randn(samples, input_size)\n",
    "    y = np.random.randint(0, num_classes, samples)\n",
    "    return X, y\n",
    "\n",
    "# Hyperparameters\n",
    "samples=3000\n",
    "input_size = 2\n",
    "num_classes=2\n",
    "hidden_size=16\n",
    "output_size=2\n",
    "learning_rate=0.01\n",
    "epochs=100\n",
    "batch_size=32\n",
    "\n",
    "X, y = generate_data(samples=samples, input_size=input_size, num_classes=num_classes)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.54871764, -0.8431569 ],\n",
       "        [ 0.07607584,  0.33348461],\n",
       "        [ 0.79725423,  1.92790822],\n",
       "        ...,\n",
       "        [-1.65017794,  1.55940009],\n",
       "        [ 0.74685988,  0.67317155],\n",
       "        [-1.49636917,  0.13821293]]),\n",
       " array([0, 1, 1, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((2400, 2), (2400,)), ((600, 2), (600,)))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=52)\n",
    "\n",
    "((X_train.shape, y_train.shape), (X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.RandomDataset at 0x200f0c39ee0>,\n",
       " <__main__.RandomDataset at 0x200f0c36030>,\n",
       " 2400,\n",
       " 600)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = RandomDataset(X_train, y_train)\n",
    "test_dataset = RandomDataset(X_test, y_test)\n",
    "\n",
    "train_dataset, test_dataset, train_dataset.__len__(), test_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5226, -0.7125]) tensor(0)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataset:\n",
    "    print(X, y)\n",
    "    print(X.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5226, -0.7125]) torch.float32 tensor(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<__main__.RandomDataset at 0x200f0c39ee0>,\n",
       " <__main__.RandomDataset at 0x200f0c36030>)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for X, y in train_dataset:\n",
    "    print(X, X.dtype, y)\n",
    "    break\n",
    "\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleANN(\n",
       "  (fc1): Linear(in_features=2, out_features=16, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleANN(input_size=input_size, hidden_size=hidden_size, output_size=output_size).to(device=device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CrossEntropyLoss(),\n",
       " Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.01\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ))"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([-1.5226, -0.7125]), tensor(0)), 75, 2400)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0], len(train_loader), len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_function, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_function(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch%200==0:\n",
    "            loss, current = loss.item(), (batch+1)*len(X)\n",
    "            print(f\"Loss: {loss}, current: {current}/{size}\")\n",
    "\n",
    "\n",
    "def inference(dataloader, model, loss_function):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            _, preds = torch.max(pred, 1)\n",
    "            loss = loss_function(pred, y)\n",
    "            test_loss += loss.item()\n",
    "            correct += (preds == y).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test accuracy: {correct*100}, Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 0.7087156176567078, current: 32/2400\n",
      "Test accuracy: 45.5, Loss: 0.6977007796889857\n",
      "Epoch: 5\n",
      "Loss: 0.7003018260002136, current: 32/2400\n",
      "Test accuracy: 55.833333333333336, Loss: 0.690858498999947\n",
      "Epoch: 10\n",
      "Loss: 0.6964715123176575, current: 32/2400\n",
      "Test accuracy: 47.333333333333336, Loss: 0.697541572545704\n",
      "Epoch: 15\n",
      "Loss: 0.7018226385116577, current: 32/2400\n",
      "Test accuracy: 54.333333333333336, Loss: 0.6921732237464503\n",
      "Epoch: 20\n",
      "Loss: 0.6855232119560242, current: 32/2400\n",
      "Test accuracy: 52.83333333333333, Loss: 0.6909546977595279\n",
      "Epoch: 25\n",
      "Loss: 0.712070643901825, current: 32/2400\n",
      "Test accuracy: 53.333333333333336, Loss: 0.6917068707315546\n",
      "Epoch: 30\n",
      "Loss: 0.6675142049789429, current: 32/2400\n",
      "Test accuracy: 52.33333333333333, Loss: 0.6915348485896462\n",
      "Epoch: 35\n",
      "Loss: 0.6965911984443665, current: 32/2400\n",
      "Test accuracy: 48.66666666666667, Loss: 0.6921215590677763\n",
      "Epoch: 40\n",
      "Loss: 0.6912569999694824, current: 32/2400\n",
      "Test accuracy: 52.83333333333333, Loss: 0.6910606591325057\n",
      "Epoch: 45\n",
      "Loss: 0.686612606048584, current: 32/2400\n",
      "Test accuracy: 53.0, Loss: 0.6902688177008378\n",
      "Epoch: 50\n",
      "Loss: 0.6930556893348694, current: 32/2400\n",
      "Test accuracy: 52.666666666666664, Loss: 0.6912290610765156\n",
      "Epoch: 55\n",
      "Loss: 0.6840562224388123, current: 32/2400\n",
      "Test accuracy: 47.5, Loss: 0.696279343805815\n",
      "Epoch: 60\n",
      "Loss: 0.6766670942306519, current: 32/2400\n",
      "Test accuracy: 53.333333333333336, Loss: 0.6900962057866549\n",
      "Epoch: 65\n",
      "Loss: 0.7067739963531494, current: 32/2400\n",
      "Test accuracy: 53.333333333333336, Loss: 0.6910803223911085\n",
      "Epoch: 70\n",
      "Loss: 0.6920733451843262, current: 32/2400\n",
      "Test accuracy: 53.166666666666664, Loss: 0.690243523371847\n",
      "Epoch: 75\n",
      "Loss: 0.6839150190353394, current: 32/2400\n",
      "Test accuracy: 52.0, Loss: 0.691559540598016\n",
      "Epoch: 80\n",
      "Loss: 0.6915146112442017, current: 32/2400\n",
      "Test accuracy: 52.0, Loss: 0.691199845389316\n",
      "Epoch: 85\n",
      "Loss: 0.7012681365013123, current: 32/2400\n",
      "Test accuracy: 52.5, Loss: 0.6903702334353798\n",
      "Epoch: 90\n",
      "Loss: 0.6786879301071167, current: 32/2400\n",
      "Test accuracy: 49.333333333333336, Loss: 0.6920038618539509\n",
      "Epoch: 95\n",
      "Loss: 0.6910008788108826, current: 32/2400\n",
      "Test accuracy: 49.5, Loss: 0.6917351735265631\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    if i%5==0:\n",
    "        print(f\"Epoch: {i}\")\n",
    "        train(train_loader, model, loss_function=loss, optimizer=optimizer)\n",
    "        inference(test_loader, model=model, loss_function=loss)\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
