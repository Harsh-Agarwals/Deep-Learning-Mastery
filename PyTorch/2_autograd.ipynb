{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.autograd**: to compute gradients, pyTorch has built-in differentiation engine torch.autograd. It supports automatic computation of gradient for any computational graph\n",
    "\n",
    "We set **requires_grad** for those variables which we need to optimize. We will be able to compute the gradient of the loss function with respect to those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.]),\n",
       " tensor([[ 1.4352, -0.1994, -1.6113],\n",
       "         [-1.1986, -0.1369,  0.3532],\n",
       "         [ 0.9336,  0.0817,  0.0554],\n",
       "         [-1.6064,  0.3219, -1.8778],\n",
       "         [ 0.0423,  0.1048,  0.9700]], requires_grad=True),\n",
       " tensor([-0.1757,  0.6317,  0.8846], requires_grad=True))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "x, y, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5]), torch.Size([3]), torch.Size([5, 3]), torch.Size([3]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape, w.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), tensor([-0.5698,  0.8037, -1.2261], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z.shape, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(y, z)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients\n",
    "- use loss.backward(), then retrieve gradient values from w.grad and b.grad\n",
    "- .grad is only available for the leaf node, having requires_grad=True\n",
    "\n",
    "- we can only perform .backward() once on a given graph, for performance reasons. If we need to do several .backward calls on the same graph, we need to pass **retain_graph=True** to the .backward call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0., -0., -0.],\n",
       "         [-0., -0., -0.],\n",
       "         [-0., -0., -0.],\n",
       "         [-0., -0., -0.],\n",
       "         [-0., -0., -0.]]),\n",
       " tensor([-0., -0., -0.]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disabling gradient tracking\n",
    "\n",
    "Use with **torch.no_grad():**\n",
    "\n",
    "Used when:\n",
    "- During eval\n",
    "- To mark some parameters as frozen\n",
    "- speeding up the computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.5698,  0.8037, -1.2261], grad_fn=<AddBackward0>), True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "z, z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.5698,  0.8037, -1.2261]), False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "\n",
    "z, z.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**autograd keeps a record of data(tensors) and all executed operations(along with the new results) in a DIRECTED ACYCLIC GRAPH(DAG) consisting of Function objects**\n",
    "\n",
    "- DAG leaves: input tensors\n",
    "- DAG roots: output tensors\n",
    "\n",
    "Computing gradients => tracing graph from root to leaves\n",
    "\n",
    "**Each graph is recreated from scratch in each .backward() call (So dynamic DG)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
