{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data using Langchain\n",
    "### langchain_community.document_loaders\n",
    "#### Loading text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x186056e58b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"doc.txt\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Document(metadata={'source': 'doc.txt'}, page_content='Hey, welcome to this document.')],\n",
       " Document(metadata={'source': 'doc.txt'}, page_content='Hey, welcome to this document.'),\n",
       " list,\n",
       " langchain_core.documents.base.Document)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = loader.load()\n",
    "doc, doc[0], type(doc), type(doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x186058f1f70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path=\"./Essay.pdf\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './Essay.pdf', 'page': 0}, page_content=\"Why one should write?  \\n \\nWhen we look into the lives of some of the most successful leaders, ancient philosophers, innovators, \\nphilanthropists from ancient times to modern times, there is one quality that distinguishes them from \\nthe rest: Clarity of thought!  It is this clarity of thought that helps them build great organizations, \\nlead people, bring new innova tions and inspire generations.  \\nSo the ultimate question revolves around how they became so clear with their thoughts? Steve Jobs \\nwas very clear about what he wanted his produ cts to be and what kind of people he wanted in his \\norganization. Chanakya was very clear with his vision of a united India and its leadership. Lee Kuan \\nYew, a brilliant statesman, was clear with his vision of a modern and economic powerhouse island \\nstate, Singapore. Modi is clear with his vision of India -2047 in 2022. Elon Musk was clear with his \\nvision of reusable rockets.  \\nAchieving clarity is a compounding process that comes from experience and learning. All of these \\nindividuals have been avid readers, w riters  and explorers . The accumulation of thoughts over years \\nshaped their vision and their actions changed the trajectory of their destination. Peter Thiel ran The \\nStanford Review  while at Stanford, which publishes rational thoughts, ideologies, visions, technologies \\nand ideas about the future. Almost all of the editors of this magazine are great founders or serve on \\nthe board of the most successful companies of our time, making them highly successful and \\ninfluential individuals. This one magazine made the  editors study tech, law, history, economics, arts \\nand philosophy, making them critical thinkers and the best of all trades! Paul Graham still writes, \\nSam Altman still writes, Ben Horowitz still writes and all those shaping the future still writes. Writing  \\nsimply amplifies your thoughts!  \\nReference 1/ https://fortune.com/2023/08/24/peter -thiel-student-newspaper -stanford -review/  \\nIf you write, you will read, if you read, you will gain knowledge, if you gain knowledge, you will \\nunderstand the world, and if you understa nd the world, you will have solutions to its problems and, \\nmore importantly, clarity of vision for their understanding and implementation. It forces you to think. \\nIt forces you to act. It forces you to change.  \\nReference 2/ https://www.paulgraham.com/read.html  \\nJust like meditation helps with concentration, writing sharpens critical thinking . If we want to \\ncontribute positively to this world, we need to become great thinkers, good listeners and great \\nexecutors. This is impossible without clarity of thought and critica l thinking. And it all starts with \\nwriting.  \\nWhen you write on a particular topic, you engage deeply with it, you give it a thought, you study the \\ntopic well, you go through opinions on it, you add your perspective, and wow, you come up with your \\nown views and rational logic on the topic, be it political, philosophical, technological, historical or \\nanything. Now you have something of your own(even though it's just a thought) and it's so strong that \\nit will force you to think more, read more, explore more and  have more of your own. Once you start \\nhaving your own thoughts, it will lead to a rational uninfluenced thinking, a particular thought \\nprocess and a winning mentality. And that day won't be far when you'll have some creation of your \\nown! \\nNot just critica l thinking and clear thoughts, writing will help you connect to the world, communicate \\nbetter, make you smarter, disciplined, have ocean of knowledge and a stronger memory.  \\nReference 3/ \\nhttps://x.com/pritika_mehta/status/1790799256783393204?t=qosur91exNY43VLf0qzOdg& s=08  \\n \\n@silver_cule on X  \")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = loader.load()\n",
    "pdf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web based loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1861e7ebf80>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path=\"https://ai.google/\", \n",
    "    show_progress=True,\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "        class_=(\"home-banner\", \"glue-grid\")\n",
    "    ))    \n",
    "    )\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://ai.google/'}, page_content=\"\\n\\n\\n\\nMaking AI helpful for everyone\\n\\n          We're committed to improving the lives of as many people as possible. And we’ll continue to responsibly build products and platforms – powered by the most advanced technology – for billions of people around the world.\\n        \\n\\n\\n\\n\\n\\n\\n                The Gemini ecosystem brings together our models, products and platforms.\\n              \\n\\n\\n\\n                Gemini is evolving to be more than just the models. It supports an entire ecosystem — from products to the APIs and platforms helping developers and businesses innovate.\\n              \\n\\n\\n\\n                  Discover the Gemini ecosystem\\n                \\n\\n\\n\\n\\n            AI that helps people do what they care about most\\n          \\n\\n\\n\\n            From sparking your creativity to boosting productivity and enriching knowledge & learning, AI makes our products even more helpful.\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n              Bringing AI to our platforms\\n            \\n\\n                Google AI on Android reimagines your mobile device experience, helping you be more creative, get more done, and stay safe with powerful protection from Google. Just circle an image, text, or video to search anything across your phone with Circle to Search* and learn more with AI-powered overviews. Make complex edits without pro-level editing skills, with Magic Editor. Enjoy a personalized, smarter messaging experience with Magic Compose.\\n            \\n\\n\\n                  Explore Google AI on Android\\n                \\n\\n\\n                *Available on select devices and internet connection required. Works on compatible apps and surfaces. Results may vary depending on visual matches and are for illustrative purposes only. Sequences are shortened.\\n              \\n\\n\\n\\n\\n            Helping developers solve complex challenges\\n          \\n\\n\\n\\n            We enable developers by providing the building blocks, tools, and resources to solve complex challenges and build innovative solutions with AI.\\n          \\n\\n\\n\\n\\n            Enabling organizations to grow and innovate\\n          \\n\\n\\n\\n            We enable organizations to use AI to work smarter, make better decisions, leverage powerful tools to streamline operations, gain deeper insights, and bring innovative ideas to life faster.\\n          \\n\\n\\n\\n\\nResponsible AI is woven into the fabric of our work\\n\\n\\n\\n\\nWe approach AI boldly and responsibly, working together with experts, partners and other organizations  so our models, products and platforms can be safer, more inclusive, and benefit society.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Guided by principles: our foundational approach to AI development.\\n              \\n\\n\\nLearn more\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Our AI innovations are built using our responsible AI practices and led by teams at Google DeepMind and Google Research.\\n              \\n\\n\\nLearn more\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Guided by principles: our foundational approach to AI development.\\n              \\n\\n\\nLearn more\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Our AI innovations are built using our responsible AI practices and led by teams at Google DeepMind and Google Research.\\n              \\n\\n\\nLearn more\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAlphaFold\\n\\n\\nProject Relate\\n\\n\\nConnectomics\\n\\n\\nGraphcast\\n\\n\\n\\n                  See more\\n                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAlphaFold is accelerating research in nearly every field of biology\\n\\n\\nBy solving a decades-old scientific challenge, Google DeepMind’s AlphaFold gave millions of researchers a powerful new tool to help solve crucial problems like discovering new medicines or breaking down single-use plastics. One day, it might even help unlock the mysteries of how life itself works.\\n\\n\\n\\n\\nLearn more\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProject Relate: communication made easier\\n\\n\\nAround the world, it is estimated that 250,000,000 people have non-standard speech. Project Relate is a beta Android application that offers personalized speech recognition to empower people in their everyday lives.\\n\\n\\n\\n\\nLearn more\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelping map the human brain using connectomics\\n\\n\\nMapping the entire human brain could help us understand a lot about ourselves, from the causes of diseases to how we store memories. But mapping the brain with today’s technology would take billions of dollars and hundreds of years. Learn what Google Research is doing to make it easier for scientists to—someday—reach this goal.\\n\\n\\n\\n\\nLearn more\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGraphcast delivers 10-day weather predictions at unprecedented accuracy\\n\\n\\nGoogle DeepMind’s state-of-the-art AI model provides faster, more accurate medium-range weather forecasts than the current industry standard, including earlier warnings for extreme weather events like cyclones, floods, and temperature spikes.\\n\\n\\n\\n\\nLearn more\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n              Our AI journey\\n            \\n\\n                Learn how Google has worked over the past 20 years to make AI helpful for everyone.\\n            \\n\\n\\n                  Explore\\n                \\n\\n\\n\\n\\nExplore our other teams and product areas\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Google DeepMind\\n                  \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Google Research\\n                  \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Google Cloud\\n                  \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    LABS.GOOGLE\\n                  \\n\\n \\n\\n\\n\\n\\n\\n\")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_content = loader.load()\n",
    "web_content[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arxiv papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.arxiv.ArxivLoader at 0x1861e87a9f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(query=\"2408.13634\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2024-08-24', 'Title': 'Enhanced Astronomical Source Classification with Integration of Attention Mechanisms and Vision Transformers', 'Authors': 'Srinadh Reddy Bhavanam, Sumohana S. Channappayya, P. K. Srijith, Shantanu Desai', 'Summary': \"Accurate classification of celestial objects is essential for advancing our\\nunderstanding of the universe. MargNet is a recently developed deep\\nlearning-based classifier applied to SDSS DR16 dataset to segregate stars,\\nquasars, and compact galaxies using photometric data. MargNet utilizes a\\nstacked architecture, combining a Convolutional Neural Network (CNN) for image\\nmodelling and an Artificial Neural Network (ANN) for modelling photometric\\nparameters. In this study, we propose enhancing MargNet's performance by\\nincorporating attention mechanisms and Vision Transformer (ViT)-based models\\nfor processing image data. The attention mechanism allows the model to focus on\\nrelevant features and capture intricate patterns within images, effectively\\ndistinguishing between different classes of celestial objects. Additionally, we\\nleverage ViTs, a transformer-based deep learning architecture renowned for\\nexceptional performance in image classification tasks. We enhance the model's\\nunderstanding of complex astronomical images by utilizing ViT's ability to\\ncapture global dependencies and contextual information. Our approach uses a\\ncurated dataset comprising 240,000 compact and 150,000 faint objects. The\\nmodels learn classification directly from the data, minimizing human\\nintervention. Furthermore, we explore ViT as a hybrid architecture that uses\\nphotometric features and images together as input to predict astronomical\\nobjects. Our results demonstrate that the proposed attention mechanism\\naugmented CNN in MargNet marginally outperforms the traditional MargNet and the\\nproposed ViT-based MargNet models. Additionally, the ViT-based hybrid model\\nemerges as the most lightweight and easy-to-train model with classification\\naccuracy similar to that of the best-performing attention-enhanced MargNet.\"}, page_content='Enhanced Astronomical Source Classification with\\nIntegration of Attention Mechanisms and Vision\\nTransformers\\nSrinadh Reddy Bhavanam1, 2, Sumohana S Channappayya1,\\nSrijith P. K3, Shantanu Desai4\\n1Department of Electrical Engineering, IIT Hyderabad, Kandi,\\nHyderabad, 502284, Telangana, India.\\n2Department of Physics and Astronomy, Clemson University, Kinard\\nLab of Physics, Clemson, 29634, South Carolina, USA.\\n3Department of Computer Science and Engineering, IIT Hyderabad,\\nKandi, Hyderabad, 502284, Telangana, India.\\n4Department of Physics, IIT Hyderabad, Kandi, Hyderabad, 502284,\\nTelangana, India.\\nContributing authors: srinadhml99@gmail.com; sumohana@ee.iith.ac.in;\\nsrijith@cse.iith.ac.in; shntn05@gmail.com;\\nAbstract\\nAccurate classification of celestial objects is essential for advancing our under-\\nstanding of the universe. MargNet is a recently developed deep learning-based\\nclassifier applied to the Sloan Digital Sky Survey (SDSS) Data Release 16 (DR16)\\ndataset to segregate stars, quasars, and compact galaxies using photometric data.\\nMargNet utilizes a stacked architecture, combining a Convolutional Neural Net-\\nwork (CNN) for image modelling and an Artificial Neural Network (ANN) for\\nmodelling photometric parameters. Notably, MargNet focuses exclusively on com-\\npact galaxies and outperforms other methods in classifying compact galaxies\\nfrom stars and quasars, even at fainter magnitudes. In this study, we propose\\nenhancing MargNet’s performance by incorporating attention mechanisms and\\nVision Transformer (ViT)-based models for processing image data. The attention\\nmechanism allows the model to focus on relevant features and capture intri-\\ncate patterns within images, effectively distinguishing between different classes of\\ncelestial objects. Additionally, we leverage ViTs, a transformer-based deep learn-\\ning architecture renowned for exceptional performance in image classification\\ntasks. We enhance the model’s understanding of complex astronomical images by\\n1\\narXiv:2408.13634v1  [astro-ph.IM]  24 Aug 2024\\nutilizing ViT’s ability to capture global dependencies and contextual information.\\nOur approach uses a curated dataset comprising 240,000 compact and 150,000\\nfaint objects. The models learn classification directly from the data, minimizing\\nhuman intervention. Furthermore, we explore ViT as a hybrid architecture that\\nuses photometric features and images together as input to predict astronomical\\nobjects. Our results demonstrate that the proposed attention mechanism aug-\\nmented CNN in MargNet marginally outperforms the traditional MargNet and\\nthe proposed ViT-based MargNet models. Additionally, the ViT-based hybrid\\nmodel emerges as the most lightweight and easy-to-train model with classifica-\\ntion accuracy similar to that of the best-performing attention-enhanced MargNet.\\nThis advancement in deep learning will contribute to greater success in identify-\\ning objects in upcoming surveys like the Vera C. Rubin Large Synoptic Survey\\nTelescope.\\nKeywords: Photometric catalogues, Classification, Astronomical image processing,\\nConvolutional neural networks, Attention mechanism, and Vision transformer.\\n1 Introduction\\nClassifying celestial objects is a longstanding challenge in observational astronomy.\\nThe segregation between stars, quasars, and galaxies in astronomical images is pivotal\\nto harnessing the best science out of the data. Efficient and robust ways of classification\\nhave become of topical importance due to the plethora of ongoing and upcoming\\nastronomical surveys such as the Dark Energy Survey (DES)- (Dark Energy Survey\\nCollaboration et al, 2016), the Sloan Digital Sky Survey (SDSS)-(York et al, 2000),\\nthe Zwicky Transient Facility (ZTF)-(Bellm, 2014), Euclid (Euclid Collaboration et al,\\n2022), the Subaru Prime Focus Camera (Miyazaki et al, 2002), etc. The upcoming\\nVera C. Rubin Observatory’s LSST (Ivezi´c et al, 2019) will further contribute to this\\ndata deluge, collecting approximately 15 terabytes of data every night. Given this large\\nvolume of data, manual analysis becomes impractical, necessitating the development\\nof automated tools for accurate classification of astronomical sources. These tools are\\ncrucial for advancing our understanding of the universe, especially cosmology and\\nstructure formation. A more detailed discussion of the impact of source separation on\\ncosmological analyses can be found in (Soumagnac et al, 2015).\\nOver the past two decades, the evolution of Machine Learning (ML), especially\\nDeep Learning (DL), has significantly impacted astronomy (Ball and Brunner, 2010;\\nBaron, 2019; Lahav, 2023). ML, with its data-driven training for learning a task,\\nand DL, utilizing multi-layer neural network architectures to acquire hierarchical\\nfeature representations, have revolutionized tasks in many areas of astrophysics. A\\nnon-exhaustive list of applications of ML to astrophysics includes the classification of\\nstellar spectra (Kuntzer et al, 2016; Sharma et al, 2020a,b), photometric light curves\\n(Lochner et al, 2016; Mahabal et al, 2019; M¨oller and de Boissi`ere, 2020), and galaxy\\nmorphologies\\n(Barchi et al, 2020; Gupta et al, 2022), photometric redshift estima-\\ntion\\n(Pasquet et al, 2019), gravitational wave analysis (George and Huerta, 2018),\\nidentification of strong lenses (Cheng et al, 2020), separation of pulsars signals from\\n2\\nradio frequency interference (Bethapudi and Desai, 2018), etc. Moreover, DL algo-\\nrithms have been shown to be very effective in detecting contaminants in astronomical\\nimages such as cosmic ray hits (Zhang and Bloom, 2020; Xu et al, 2023; Bhavanam\\net al, 2022a,b), spurious reflections (“ghosts”), and light scattering (Tanoglidis et al,\\n2022; Chang et al, 2021). This integration enhances efficiency and accuracy in various\\nastronomical tasks, showcasing the ongoing potential for data-driven advancements in\\nthe field.\\nThe growing availability of data has elevated ML as a powerful tool for astronomical\\nsource classification tasks. In the past, methods like Random Forest (RF)- (Vasconcel-\\nlos et al, 2011), Support Vector Machines (SVM)- (Fadely et al, 2012), and Gradient\\nBoosting classifiers (L´opez-Sanjuan et al, 2019) were favoured for their ease of train-\\ning with limited data but suffered from weaker performance, typically achieving an\\naccuracy between 80% and 90% for the star-galaxy separation. More recent approaches\\nutilizing Convolutional Neural Networks (CNNs)- (Kim and Brunner, 2017; Hao-ran\\net al, 2017) have yielded significantly improved results. These advancements were facil-\\nitated by the ML community’s progress in developing enhanced image classification\\nalgorithms, as exemplified by the ImageNet dataset (Krizhevsky et al, 2012; Simonyan\\nand Zisserman, 2014).\\nStar-galaxy classification is pivotal to achieving the science requirements from\\nongoing and next-generation photometric surveys. Although many metrics based\\non point-spread function (PSF) information have been used for star-galaxy separa-\\ntion (Desai et al, 2012; Slater et al, 2020), they cannot easily separate stars and\\nquasars. Therefore, many works have explored star-galaxy-quasar separation using\\nML. While some earlier studies addressed the entire star, quasar and galaxy separa-\\ntion problem using classical ML and DL algorithms (Odewahn et al, 1992; Nakazono\\net al, 2021; Wen and Yang, 2021; Wang et al, 2022), it is essential to note that there\\nis often a faint limit in source classification, beyond which accurate categorization\\nbecomes challenging. For instance, current methods experience performance degrada-\\ntion when dealing with objects fainter than r > 22.5 (Cabayol et al, 2019) and i > 21\\n(Kim and Brunner, 2017). With the advent of new sky surveys like the Vera Rubin\\nLSST, which observe even fainter celestial objects, achieving accurate classification at\\nthese faint magnitudes becomes increasingly paramount.\\nIn this study, we leverage DL to distinguish stars, quasars, and compact galaxies\\nat faint magnitudes, where stars and quasars appear as point sources convolved with\\nthe PSF. On the other hand, galaxies are extended sources. Traditionally, differentiat-\\ning stars and quasars relied on spectra, a resource-intensive process. ML, particularly\\ncolor-based methods, provides an efficient alternative\\n(Abraham et al, 2018) for\\nthe star-quasar classification. However, distinguishing high-redshift quasars remains\\nchallenging due to intrinsic color variations. Morphological approaches for galax-\\nies (Sebok, 1986) face limitations with fainter and more compact objects, prompting\\nthe exploration of spectral energy distributions, although hindered by data avail-\\nability. Our investigation extends to studying attention-augmented CNNs and Vision\\nTransformer (ViT) architectures, offering potential enhancements for image-based\\nsource classification tasks in astronomy and addressing challenges posed by traditional\\nmethods.\\n3\\nRecently, a novel DL framework called “MargNet” (Chaini et al, 2023) (C23,\\nhereafter) was introduced for classifying stars, quasars and compact galaxies at faint\\nmagnitudes with enhanced accuracy. This method employs a DL architecture that\\ncombines photometric features-based and image-based classification, using a stacking\\nensemble of an Artificial Neural Network (ANN) and a CNN, respectively. The model’s\\ntraining dataset incorporates photometric features and images in five photometric\\nbands (u, g, r, i, z) from the SDSS Data Release (DR)16 (Ahumada et al, 2020). The\\nobjects have been classified as stars, quasars or galaxies spectroscopically. Selection\\ncriteria outlined in Section 2 of C23 ensured the inclusion of compact galaxies and\\nfaint sources by applying constraints on the de Vaucouleur radius and photometric\\nmagnitude. Their study conducted three experiments to assess the performance in the\\nfaint and compact regime, emphasizing the importance of precise source separation.\\nThis work addresses the classification challenge of faint and compact astronom-\\nical objects by enhancing the “MargNet” model architecture initially used in C23.\\nWe extend our investigation to incorporate attention-enhanced CNNs and ViT archi-\\ntectures to better handle image data. In particular, we integrate the Squeeze and\\nExcitation (SE)-(Hu et al, 2018) attention module into CNNs, a technique that has\\npreviously shown substantial performance improvements in image classification tasks.\\nTherefore, this work is an extension of the analysis carried out in C23, where we use the\\nsame dataset and analysis cuts, ensuring that the evaluation of the new enhancements\\nremains consistent and comparable to the previous findings.\\nFirst introduced by (Dosovitskiy et al, 2020), ViTs excel in image classification,\\nobject detection (Li et al, 2022), and semantic segmentation (Chen et al, 2021) tasks.\\nViTs represent a revolutionary paradigm in computer vision, diverging from traditional\\nCNNs by adopting self-attention mechanisms. Their unique approach to processing\\nimage patches through self-attention allows them to capture global dependencies and\\nintricate spatial relationships, leading to state-of-the-art performance. ViTs have also\\nproven effective in transfer learning, particularly in scenarios with limited labelled\\ndata, and found applications in medical image analysis (He et al, 2023), remote sens-\\ning (Bazi et al, 2021), and various domains where understanding global context is\\ncritical.\\nWhile attention-enhanced CNNs have improved performance over baseline CNN\\nmodels, their application in astronomy remains relatively unexplored, except for con-\\ntaminant detection (Bhavanam et al, 2022a). Although ViTs have been investigated\\nfor galaxy morphology classification (Karpoor, 2022; Lin et al, 2021), their potential\\nto address the source separation problem remains untapped. Further, by leveraging the\\ncapabilities of ViT, we aim to extract and analyze intricate features from diverse astro-\\nnomical data types, including photometric features and FITS images. This approach\\nfacilitates the identification of unique signatures associated with various classes of\\ncelestial objects.\\nOur primary contributions in this work are described as follows:\\n• Exploring the modality-specific source classification models tailored to handle\\nphotometric features and FITS images separately.\\n4\\n• Investigating the optimal approach to source classification using FITS images solely,\\ncomparing the performance of CNNs with and without attention augmentation and\\nViT-based models.\\n• Unraveling the “MargNet” model and expanding its capabilities by integrating\\nattention-enhanced CNNs and ViT models for the image-based classifier.\\n• Exploring the synergistic potential of integrating photometric features and FITS\\nimages through fusion in ViT-based architectures. This augmentation aims to\\nenhance astrophysical source detection capabilities within the baseline ViT frame-\\nwork.\\nThe remainder of this paper is structured as follows: Section 2 covers data acquisi-\\ntion from the SDSS DR16 Catalogues. Section 3 describes the preprocessing steps, the\\nmethods employed for the classification problem and details on the models. In Section\\n4, we present the results and discussion; in Section 5, we present the conclusions and\\nfurther possibilities arising from this work.\\n2 Dataset and Experiments\\nThe primary objective of this study is to reliably classify the astrophysical sources with\\nfaint and compact characteristics. For this study, we rely on SDSS DR 16 (Ahumada\\net al, 2020), specifically utilizing SDSS photometric data in the u, g, r, i, and z\\npassbands (Fukugita et al, 1996) as model inputs. Spectroscopically assigned classes\\nfrom the SDSS pipeline serve as labels for supervised training, ensuring the accuracy\\nof our model predictions. We use both photometric features and FITS images from\\nthe SDSS DR 16 for our analysis.\\n2.1 Compact and Faint source dataset\\nTo create a sample of galaxies which can be identified as compact, we use the same\\ncompactness parameter (c) introduced in C23 and choose the threshold of c = 0.5\\nto separate the two. To constitute the faint object dataset, we impose the following\\nconstraint on the average magnitude across the five passbands from SDSS data, similar\\nto C23:\\n⟨mag⟩> 20\\n(1)\\nMore detailed discussion of these cut choices can be found in C23. Using these\\ncuts, we create both a Compact source dataset as well as a Faint and Compact source\\ndataset using DR16 data in the same way as C23.\\nAfter retrieving the photometric features and FITS images from both the datasets,\\nwe are set up to conduct the experiments. We split the dataset into training, val-\\nidation, and test sets, ensuring an equal representation of each class. We prepared\\nthree different experiments, which we label as Experiment 1, Experiment 2, and\\nExperiment 3 and the details of each of these experiments can be found below:\\n• Experiment 1: We obtain all three sets: training, validation, and test from the\\nCompact source dataset. Importantly, the test set is representative of the training\\nset in this context.\\n5\\n• Experiment 2: The training, validation, and test sets are drawn from the Faint\\nand Compact source dataset.\\n• Experiment 3: The training and validation sets are selected from the Compact\\nsource dataset. However, the test set is chosen from the Faint and Compact source\\ndataset.\\nFurther details about these experiments on data splitting are exactly the same as C23,\\nand we omit the details for brevity.\\n3 Methodology\\nThe primary objective of this work is to explore optimal models for source classifica-\\ntion, utilizing both the photometric features and FITS images. Our study includes the\\nindependent and joint analysis of models leveraging these two types of features.\\n3.1 Classification using Photometric Features\\nWe utilized 24 distinct features, as outlined in Table 1 of C23, to represent the pho-\\ntometric information derived from celestial source images. This phase capitalizes on\\nthe wealth of photometric features to extract insights into celestial objects, encom-\\npassing brightness, color, and variability. The classification task using photometric\\nfeatures involves employing classical supervised ML classifiers alongside ANNs. ML\\nclassifiers, including Decision Trees (DT), RF, and XGBoost (XGB), play a pivotal\\nrole in classifying entities such as star-galaxy-quasars. Furthermore, adhering to the\\nmodel proposed in C23, incorporating ANN classifiers offers an additional approach\\nfor achieving precise and accurate classification of celestial objects.\\n1. Decision Trees, a prevalent ML technique in astronomy for tasks like classi-\\nfication and regression, operate by recursively partitioning the data based on feature\\nconditions and establishing a tree (Quinlan, 1986). Each split in the tree represents a\\ndecision, and collectively, they form a hierarchical structure. DTs are known for their\\ninterpretability and simplicity, enabling clear insights into decision-making. However,\\nthey may be prone to overfitting and capturing noise in the training data. Regular-\\nization techniques, ensemble methods like RF, and careful tuning help mitigate these\\nissues, making DTs valuable in astronomical analysis (Vasconcellos et al, 2011).\\n2. Random Forest, a widely used ML technique in astronomy (Baron, 2019),\\ncreates ensembles of DTs through bootstrap resampling, preventing overfitting by\\nleveraging diverse subsets of features during training. Introduced in Breiman (2001),\\nRF excels in efficiency, scalability, and competitive performance for classification and\\nregression in astronomy compared to other ML algorithms.\\n3. XGBoost (XGB), an influential ML algorithm widely applied in astronomy\\nfor tasks such as classification and regression\\n(Bethapudi and Desai, 2018; Zhang,\\n2022), stands out for its exceptional predictive performance. An optimized gradient\\nboosting framework, XGB sequentially builds a strong predictive model by combining\\nweak learners (typically DTs). It incorporates regularization techniques to control\\nmodel complexity and prevent overfitting (Chen and Guestrin, 2016), and it includes\\nadvanced features like parallel processing and handling missing values. Renowned for\\n6\\nFig. 1: The Inception module architecture used.\\nits speed, efficiency, and ability to handle large datasets, XGBoost has become popular\\nin astronomy for achieving accurate and robust predictions.\\n4. Artificial Neural Networks, inspired by biological neural networks, ANNs\\nemulate biological neurons, receiving inputs, processing information, and producing\\noutputs. The ANN model in this study, (same as in C23), is a feed-forward network\\nwith stacked dense layers activated by ReLU units. Five layers provide ample depth\\nfor directly classifying astronomical sources from selected parameters. Dropouts (Sri-\\nvastava et al, 2014) with a 0.25 fraction are employed to combat overfitting in deep\\nnetworks. The final layer, softmax-activated with three (or two) outputs for each\\ncategory, utilizes categorical (or binary) cross-entropy as the loss function for the\\nstar-quasar-galaxy classification.\\n3.2 Classification Using FITS Images\\nThe SDSS FITS images in all five passbands are preprocessed in the same way as\\nC23. We then apply the following methods to these images for astronomical source\\nseparation.\\n1. Convolutional Neural Networks are essential for image processing in computer\\nvision. Their strength lies in specialized layers like convolutional and pooling lay-\\ners, effectively capturing hierarchical image features. CNNs excel at recognizing\\npatterns, edges, and textures, enabling detailed visual analysis. Parameter sharing\\nin convolutional layers reduces model complexity, enhancing generalization across\\nimage regions. This adaptability makes CNNs robust for object recognition, image\\nclassification, and segmentation. Additionally, CNNs have been successfully applied\\nto astronomical source classification (Kim and Brunner, 2017).\\nIn C23, a CNN architecture inspired by the Inception Network, as introduced\\nby Szegedy et al (2015) for the ImageNet Challenge, is employed for compact and\\nfaint astronomical source separation. This adaptation of InceptionNet, previously\\n7\\nsuccessful in astronomy, utilizes layer sizes from Pasquet et al (2019), initially\\ndesigned for photometric redshift estimation.\\nIncluding the inception module is crucial, aiding in extracting discriminative\\nfeatures from various convolution filter sizes (1×1, 3×3, and 5×5). These filters are\\napplied at a specific depth, and their outputs are concatenated to create a consoli-\\ndated layer. We considered the same CNN model from C23 as our baseline and tried\\nto improve the performance over the baseline models across all three experiments.\\nFigure 1 illustrates the chosen inception module in this work. CNN processes pre-\\nprocessed FITS image files for data objects, providing initial classification output.\\nWith 80,000 images per class and spectroscopic labels, the CNN uses convolutional\\nlayers with varying kernel sizes ([1,1], [3,3], [5,5]) activated by ReLU units. Dense\\nlayers distil information, and the final layer employs softmax (or sigmoid) for multi-\\nclass (or binary) classification. It uses categorical cross-entropy as the loss function.\\nThe complete details of the CNN architecture used can be found in Table 3 of C23.\\n2. Attention CNN: As discussed in Section\\n1, attention mechanisms play a piv-\\notal role in computer vision in enhancing the capability of models to focus on\\nrelevant information within an image (Guo et al, 2022). Analogous to human\\nvisual attention, these mechanisms enable models to selectively emphasize specific\\nregions or features during processing. By dynamically assigning weights to dif-\\nferent input parts, attention mechanisms improve the model’s ability to capture\\nintricate patterns, relationships, and contextual information. Integrating atten-\\ntion mechanisms has become a fundamental aspect of modern computer vision\\narchitectures, contributing to overall advancement and adaptability in image-based\\ntasks. Furthermore, the attention blocks can be effortlessly incorporated into pre-\\nexisting CNN models, allowing for seamless integration without requiring extensive\\nmodifications to the architecture.\\nInspired by the success of attention-enhanced CNN models, we experimented\\nwith various attention mechanisms to enhance the performance of our baseline CNN\\nmodel for celestial object classification. In particular, we investigated the SENet\\n(Hu et al, 2018). We also explored the Simple Attention Module (SimAM; (Yang\\net al, 2021)) and Convolutional Block Attention Module (CBAM;\\n(Woo et al,\\n2018)). The SENet introduces a mechanism to adaptively recalibrate the impor-\\ntance of different channels within feature maps. The “squeeze” operation captures\\nglobal information by applying global average pooling, while the “excitation” oper-\\nation learns channel-wise dependencies. This enables SENet to dynamically assign\\nweights to channels based on relevance, enhancing the network’s discriminative\\npower and performance in tasks such as image classification and object detection.\\nThe block diagram representing operations in SENet is illustrated in Figure 2.\\nSE networks have emerged as a powerful enhancement to CNNs for image\\nclassification tasks. They aim to improve model performance by explicitly mod-\\nelling channel-wise dependencies, allowing the network to adaptively recalibrate\\nthe importance of different channels. One crucial parameter in SE networks is the\\nreduction ratio, often denoted as r. This parameter controls the dimensionality\\nof the intermediate representations within the SE block. Specifically, during the\\n8\\nFig. 2: The block diagram of the SENet used (Hu et al, 2018).\\nsqueeze phase, which involves global average pooling to obtain channel-wise statis-\\ntics, the reduction ratio determines the number of intermediate features used to\\ncapture channel dependencies. Mathematically, if the input tensor has C channels,\\nthe reduction ratio (r) reduces the number of channels to C\\nr before applying the\\nexcitation phase. Typically, r is chosen as a hyperparameter, and commonly used\\nvalues range from 2 to 16, although smaller values like 2 or 4 are prevalent.\\nChoosing an appropriate reduction ratio is crucial for balancing model com-\\nplexity and performance. A higher reduction ratio decreases computational cost\\nbut may limit the model’s capacity to capture complex channel interactions. Con-\\nversely, a lower reduction ratio may improve expressiveness but at the expense of\\nincreased computational overhead. The reduction parameter is often determined\\nthrough experimentation and validation on the specific dataset and task. It is\\nessential to balance model capacity and efficiency to achieve optimal performance.\\n3. Vision Transformers represent a paradigm shift in computer vision, leveraging\\ntransformer architectures from Vaswani et al (2017) for image classification tasks\\n(Dosovitskiy et al, 2020). Unlike traditional CNNs, ViTs treat images as sequences\\nof patches, allowing them to capture local and global contextual information. This\\napproach enables ViTs to scale efficiently to large image sizes, contributing to\\ntheir success in various image classification challenges. ViTs have demonstrated\\nstate-of-the-art performance on benchmarks such as ImageNet, showcasing their\\npotential to outperform traditional CNNs in certain scenarios. On a related note,\\nLinformer (Wang et al, 2020) is a variant of the ViT architecture that specifically\\naddresses the quadratic complexity of the self-attention mechanism in ViT, making\\nit more computationally efficient. While ViTs have excelled in image classification\\ntasks, Linformer’s advancements in attention mechanisms within ViTs make it\\nparticularly relevant for applications requiring the processing of extensive image\\ndatasets. ViTs and Linformer demonstrate the adaptability of transformer-based\\narchitectures in handling diverse image-related tasks, significantly contributing to\\nthe evolution of machine learning methodologies in computer vision.\\nThe Classification (CLS) token in the ViT architecture is a pivotal element at\\nthe model’s beginning. It serves as a representative summary of the entire input\\nimage, capturing global contextual information. Traditionally initialized randomly\\nduring training, some approaches consider deriving the CLS token intentionally\\nfrom specific features. This is studied specifically when multimodal data is available\\n9\\nFig. 3: The ViT architecture, with its randomly chosen CLS token, represents an\\nimage-based model. Conversely, deriving the CLS token using photometric features\\nrepresents a novel architectural approach, seamlessly fusing photometric features and\\nimages.\\nin remote sensing applications (Roy et al, 2023). This token is crucial in aggregat-\\ning essential information for downstream tasks, contributing significantly to ViT’s\\nvisual recognition and understanding performance. In Figure 3, the architecture of\\nViT is illustrated, and the CLS token is extracted randomly while working with\\nimages solely.\\n3.3 Classification Using Photometric Features and FITS\\nImages\\nWe systematically explored hybrid models that combine photometric features with\\nimages, investigating various integration approaches. Our experimentation involved\\ndeploying stacking ensemble models, similar to MargNet, which were trained sepa-\\nrately using photometric features and images. Later, these models were combined using\\nensemble techniques. The second idea is to implement nuanced adjustments within\\n10\\nthe ViT architecture (proposed). This included training a unified model that lever-\\nages both photometric features and images. Specifically, we adopted the multi-modal\\nfusion-based Vision Transformer (MM ViT) paradigm, inspired by the insights intro-\\nduced by Roy et al (2023). This comprehensive investigation aimed to optimize the\\nfusion of visual and photometric information to enhance model performance. Further\\ndetails on these approaches are provided below.\\n1. Ensemble Model (MargNet): The CNN (for images) and ANN (for photometric\\nfeatures) models were trained using an early stopping criterion. These pre-trained\\nmodels were then integrated in parallel through stacking to create the final net-\\nwork, MargNet. The models were fine-tuned for their inputs, eliminating the need\\nfor retraining MargNet models. CNN and ANN outputs, generated from images\\nand photometric data of each class, were used as inputs for the ensemble, in the\\nsame way as C23, where more details can be found. We also explored alternative\\narchitectures for MargNet by replacing the CNN model with attention-augmented\\nCNNs and ViTs, maintaining the ANN model unchanged.\\n2.\\nMultiModal ViT (MM ViT): This represents a pioneering approach in com-\\nputer vision by seamlessly integrating visual information from diverse modalities\\n(Roy et al, 2023). Drawing inspiration from ViT’s success in image classification,\\nMM ViT extends its capabilities to concurrently handle various image modalities,\\nsuch as multi-spectral and hyper-spectral data. Unlike traditional models that pro-\\ncess each modality separately, MM ViT employs a unified architecture, enabling\\njoint learning and information fusion. This framework empowers the model to cap-\\nture intricate relationships and dependencies across different modalities, leading\\nto more robust and holistic representations. By leveraging self-attention mecha-\\nnisms and hierarchical feature extraction, MM ViT showcases the potential for\\nenhanced performance in tasks requiring synthesising information from various\\nsources, making it a versatile and powerful tool in multimodal learning scenarios.\\nOur study treated photometric features and images as distinct modalities and\\nconducted experiments to fuse them seamlessly. We strategically designated the\\nCLS token from the ViT architecture as a pivotal hyperparameter to achieve\\nthis. Traditionally, the CLS token in ViT training is selected randomly. However,\\nour methodology advocates deriving the CLS token from photometric features.\\nSimultaneously, the model receives images as input in patch form, facilitating the\\ncomprehensive incorporation of both data types. This approach contributes to a\\nmore refined and comprehensive understanding of the combined visual and pho-\\ntometric information. The architecture of the proposed MM ViT is illustrated in\\nFigure 3. The CLS token is extracted by passing the photometric features through\\na simple ANN model. The final dense layer (with d neurons) dimension is adjusted\\nto match the dimension of the flattened image patches. Finally, the CLS token is\\nconcatenated with the projected and flattened patches before being fed into the\\nViT encoder.\\n11\\n3.4 Performance Evaluation\\nTo assess the effectiveness of classification models, it is essential to utilize three cor-\\nnerstone metrics: Precision, Recall, and Accuracy. Precision measures the proportion\\nof correctly identified positive instances (TP) out of all instances classified as positive\\n(TP + FP), providing insight into the model’s ability to avoid false positives (FP).\\nConversely, to prove insight into the model’s ability to avoid false negatives (FN),\\nRecall evaluates the model’s capability to capture all positive instances by calculat-\\ning the ratio of true positives (TP) to the total number of positive instances (TP +\\nFN). Accuracy, another crucial metric, measures the accuracy of the model’s predic-\\ntions by calculating the ratio of correctly classified instances (TP + TN) to the total\\nnumber of instances. Together, these metrics offer a comprehensive understanding of a\\nmodel’s performance across different aspects of classification tasks, thereby facilitating\\ninformed decision-making in model selection and optimization.\\nFor a specific class c, precision (Pc) and recall (Rc) are calculated as follows:\\nPc =\\nTPc\\nTPc + FPc\\n,\\nRc =\\nTPc\\nTPc + FNc\\n,\\n(2)\\nwhere c denotes objects for that particular class.\\nWe compute the aggregate precision (P) and recall (R) by averaging the preci-\\nsion (Pc) and recall (Rc) values across all classes. Precision for galaxies signifies the\\nclassifier’s proficiency in avoiding misclassifying stars/quasars as galaxies. In contrast,\\nrecall for galaxies denotes the classifier’s effectiveness in identifying all galaxies in the\\ndataset.\\nAccuracy can be calculated for a single class or all classes combined together, and\\nis given by:\\nAccuracy =\\nTP + TN\\nTP + TN + FP + FN ,\\n(3)\\nwhere TN denotes the true negatives.\\n4 Results and Discussion\\nFor each experiment described in Section 2.1, we investigate two scenarios: star-galaxy\\nclassification and star-galaxy-quasar classification. Subsequent sections delve into the\\nspecifics of each methodology, focusing on the corresponding data type and the quan-\\ntitative performance metrics. We emphasize the results obtained from models utilizing\\nphotometric features and FITS images as input.\\n4.1 Performance Analysis with Photometric Features\\nThe photometric features extracted from the source images exclusively serve as inputs\\nfor models that do not incorporate image data. We utilized classical ML algorithms,\\nincluding DT, RF, and XGB, alongside an ANN model to accommodate these explicit\\nfeatures. We implemented 10-fold cross-validation while implementing these algo-\\nrithms to ensure robust classification of both star-galaxy and star-galaxy-quasar\\n12\\nclassification. Errors and performance metrics were computed for each fold, with aver-\\nages and errors obtained from standard deviations reported. Since this analysis does\\nnot consider images, no additional preprocessing is necessary apart from standard\\ndata normalization. The performance of each model is summarized in Table 1, and our\\ndeep learning models are implemented using the PyTorch framework. Table 1 shows\\nthat XGB consistently outperforms DT and RF classifiers across all experiments and\\nclassification settings. The best performance is achieved by either XGB or ANN. For\\nthe ANN model, we employed a dropout fraction of 0.25, and the model comprises\\n331,331 neurons for the architecture employed.\\nTable 1: Quantitative performance of the classical ML algorithms and ANN model\\nwith photometric features. Models in italics represent re-trained models from C23\\nin the PyTorch framework. The best-performing models are marked in bold. The\\noptimized parameters for the DT classifier are; criterion: gini, max depth: 10,\\nmin samples split: 10. The RF has max depth=50, min samples split=5. For\\nGBDT, max depth=50, n estimators=100. While employing DT, RF, GBDT and\\nANN models, we utilized 10-fold cross-validation. Performance metrics were averaged\\nacross test folds, with errors estimated from the standard deviation.\\nExperiment\\nClasses\\nModel\\nAccuracy (%)\\nPrecision (%)\\nRecall (%)\\nExperiment 1\\n1. Star-Galaxy\\nDT\\n96.9 ± 0.1\\n96.9 ± 0.1\\n96.9 ± 0.1\\nRF\\n97.7 ± 0.1\\n97.7 ± 0.1\\n97.7 ± 0.1\\nXGB\\n98.0 ± 0.1\\n98.0 ± 0.1\\n98.0 ± 0.1\\nANN\\n97.9 ± 0.1\\n97.9 ± 0.1\\n97.9 ± 0.1\\n2. Star-Galaxy-Quasar\\nDT\\n89.5 ± 0.2\\n89.6 ± 0.2\\n89.5 ± 0.2\\nRF\\n92.0 ± 0.1\\n92.1 ± 0.1\\n92.0 ± 0.1\\nXGB\\n92.7 ± 0.2\\n92.7 ± 0.2\\n92.7 ± 0.2\\nANN\\n92.9 ± 0.1\\n92.9 ± 0.1\\n92.9 ± 0.1\\nExperiment 2\\n1. Star-Galaxy\\nDT\\n94.7 ± 0.2\\n94.7 ± 0.2\\n94.7 ± 0.2\\nRF\\n96.1 ± 0.2\\n96.1 ± 0.2\\n96.1 ± 0.2\\nXGB\\n96.3 ± 0.1\\n96.3 ± 0.1\\n96.3 ± 0.1\\nANN\\n96.2 ± 0.2\\n96.2 ± 0.2\\n96.2 ± 0.2\\n2. Star-Galaxy-Quasar\\nDT\\n81.9 ± 0.3\\n82.0 ± 0.3\\n81.9 ± 0.3\\nRF\\n85.5 ± 0.2\\n85.6 ± 0.2\\n85.5 ± 0.3\\nXGB\\n86.1 ± 0.2\\n86.1 ± 0.2\\n86.1 ± 0.2\\nANN\\n86.3 ± 0.2\\n86.4 ± 0.2\\n86.3 ± 0.2\\nExperiment 3\\n1. Star-Galaxy\\nDT\\n88.9 ± 0.3\\n90.3 ± 0.2\\n88.9 ± 0.3\\nRF\\n90.5 ± 0.2\\n91.9 ± 0.1\\n90.4 ± 0.1\\nXGB\\n92.0 ± 0.1\\n93.1 ± 0.1\\n92.0 ± 0.1\\nANN\\n92.1 ± 0.8\\n92.9 ± 0.6\\n92.1 ± 0.8\\n2. Star-Galaxy-Quasar\\nDT\\n68.9 ± 0.3\\n72.8 ± 0.2\\n68.9 ± 0.3\\nRF\\n83.1 ± 0.6\\n83.4 ± 0.6\\n83.1 ± 0.6\\nXGB\\n83.8 ± 0.5\\n83.9 ± 0.5\\n83.8 ± 0.5\\nANN\\n72.2 ± 1.8\\n74.3 ± 2.0\\n72.1 ± 1.8\\n13\\nTable 2: Quantitative performance of the proposed models using FITS images. Models\\nin italics represent the re-trained CNN model from C23, and bold are the best performed.\\nWhile using SENet added CNN, the hyperparameter ‘r’ for each experiment and class\\nsetting is mentioned in the brackets.\\nExperiment\\nClasses\\nModel\\nAccuracy (%)\\nPrecision (%)\\nRecall (%)\\nExperiment 1\\n1. Star-Galaxy\\nCNN\\n97.4 ± 0.1\\n97.4 ± 0.1\\n97.4 ± 0.1\\nSENet-CNN\\n97.5 ± 0.1\\n97.5 ± 0.1\\n97.5 ± 0.1\\n(r=2)\\nViT\\n97.1 ± 0.1\\n97.1 ± 0.1\\n97.1 ± 0.1\\n2. Star-Galaxy-Quasar\\nCNN\\n91.6 ± 0.1\\n91.7 ± 0.1\\n91.6 ± 0.1\\nSENet-CNN\\n91.9 ± 0.1\\n92.0 ± 0.1\\n91.9 ± 0.1\\n(r=32)\\nViT\\n91.2 ± 0.1\\n91.4 ± 0.1\\n91.2 ± 0.1\\nExperiment 2\\n1. Star-Galaxy\\nCNN\\n95.2 ± 0.1\\n95.2 ± 0.1\\n95.2 ± 0.1\\nSENet-CNN\\n95.6 ± 0.1\\n95.6 ± 0.1\\n95.6 ± 0.1\\n(r=16)\\nViT\\n94.2 ± 0.1\\n94.3 ± 0.1\\n94.2 ± 0.1\\n2. Star-Galaxy-Quasar\\nCNN\\n84.2 ± 0.1\\n84.5 ± 0.1\\n84.2 ± 0.1\\nSENet-CNN\\n84.8 ± 0.1\\n85.0 ± 0.1\\n84.8 ± 0.1\\n(r=32)\\nViT\\n83.7 ± 0.1\\n83.9 ± 0.1\\n83.8 ± 0.1\\nExperiment 3\\n1. Star-Galaxy\\nCNN\\n89.3 ± 0.1\\n90.3 ± 0.1\\n89.3 ± 0.1\\nSENet-CNN\\n90.1 ± 0.1\\n91.0 ± 0.1\\n90.1 ± 0.1\\n(r=2)\\nViT\\n86.4 ± 0.1\\n88.3 ± 0.1\\n86.4 ± 0.1\\n2. Star-Galaxy-Quasar\\nCNN\\n69.4 ± 0.1\\n73.9 ± 0.1\\n69.4 ± 0.1\\nSENet-CNN\\n70.5 ± 0.1\\n74.3 ± 0.1\\n70.5 ± 0.1\\n(r=32)\\nViT\\n67.9 ± 0.1\\n73.0 ± 0.1\\n67.9 ± 0.1\\n4.2 Performance Analysis with FITS Images\\nWe initially employed the inception-based CNN model from C23 as our baseline while\\nworking with the FITS images alone. We then enhanced this baseline CNN by integrat-\\ning attention mechanisms to improve its performance. Particularly, we investigated\\nthe incorporation of the SENet as detailed in Section 3.2 into our baseline model after\\neach inception module. Moreover, our study represents the pioneering exploration of\\nsource classification using attention-augmented CNNs and ViT architectures in the lit-\\nerature. Preprocessed images with dimensions of 32×32×5 serve as input for all these\\nmodels, excluding the photometric features.\\nIn our investigation of the SENet-enhanced CNN, we explored the ‘reduction (r)’\\nhyperparameter within the SENet. We experimented with values ranging from 2, 8,\\n16, and 32, studying the SENet-CNN with each ‘r’ value in separate experiments\\nand selecting the best-performing model for each case. Similarly, while exploring the\\nViT, we considered various patch sizes such as 4, 8, and 16, with a 4×4 patch size\\nproving the most effective. We implemented Linformer to alleviate the computational\\nburden, using 12 encoder layers, 8 attention heads, and 64 as the dimensions of the\\n14\\nTable 3: Image-based models with the num-\\nber of trainable parameters. The listed num-\\nbers are with three class (star-galaxy-quasar)\\nclassification settings.\\nModel (Architecture Details)\\nNo. of Parameters\\nCNN\\n25,543,783\\nSENet-CNN (r = 2)\\n26,295,335 (3 % ↑)\\nSENet-CNN (r = 8)\\n25,730,031 (0.7 % ↑)\\nSENet-CNN (r = 16)\\n25,635,607 (0.3 % ↑)\\nSENet-CNN (r = 32)\\n25,587,815 (0.1 % ↑)\\nViT\\n807,203 (96.6 % ↓)\\nflattened patches. Results are presented in Table 2 for the baseline CNN model and\\nthe proposed attention-enhanced CNN (with corresponding hyperparameter values\\nfor the best-performing model) and ViT-based models. Table 2 demonstrates that\\nthe SENet-augmented CNN outperforms the baseline CNN across all the experiments\\nand classification tasks, albeit with marginal performance gain across all the met-\\nrics. Further, the proposed SENet-CNN shows significant gains over the ViT models,\\nspecifically for Experiments 2 and 3. The proposed SENet-CNN demonstrate decent\\nperformance gain in Experiment 3, while models trained on bright sources are applied\\nto faint sources. Table 3 details the computational complexity for all image-based\\nmodels. The performance gain with the SENet-augmented CNN comes at the expense\\nof an additional 0.1% to 3% parameters. The highest performance gain in terms of\\naccuracy of around 1% is observed in Experiment 3 for both star-galaxy and star-\\ngalaxy-quasar classification, and the lowest gain of 0.1 % is noticed in Experiment 1\\ncompared to the baseline model. Conversely, the ViT model, comprising only 3.4%\\nof the parameters of the baseline CNN, achieves comparable performance across all\\nsettings, with a maximum performance drop of around 2.9% and 3.7% observed in\\nExperiment 3 for the star-galaxy-quasar classification, when compared with the base-\\nline CNN and best-performing SENet-CNN respectively. The lowest performance drop\\nof 0.3 % and 0.4 % is observed in Experiment 1 for the star-galaxy separation, com-\\npared to the baseline CNN and best-performing SENet-CNN. However, the proposed\\nViT stands out as the most lightweight image-based model for source classification.\\n4.3 Performance Analysis with Photometric Features and\\nFITS Images\\nFITS images and photometric features are provided as inputs when working with the\\nensemble models and the proposed MM ViT. Ensemble model (MargNet), comprises\\na parallel stack of photometric feature-based and image-based models. In this study,\\nalongside the baseline CNN model for images (proposed in C23), we explored the pro-\\nposed SENet-augmented CNN and ViT performance for the image-based model while\\nmaintaining the ANN fixed. These pre-trained models were then integrated in parallel\\nthrough stacking to construct the final network, MargNet. We meticulously fine-tuned\\nthe CNN and ANN models for their respective inputs, obviating the need to retrain\\n15\\nMargNet. Each class’s objects, comprising images and photometric data, are provided\\nas inputs for the CNN and ANN models. The outputs from these models subsequently\\nfeed into the ensemble. The outputs from both components are merged, and a statis-\\ntical combination of these outputs yields the final result. MargNet’s architecture at\\nconcatenation encompasses 103 neurons, and the corresponding trainable parameters\\nwith different image-based models are detailed in Table 4.\\nWhile working with the proposed MM ViT model, we incorporated photometric\\nfeatures and FITS images into the same architecture. We used the same ViT archi-\\ntecture studied for image-based classification to achieve this, as detailed in Section\\n3.2. However, a key distinction in using the MM ViT lies in the derivation of the CLS\\ntoken. Instead of relying solely on image data, we generated the CLS token for the ViT\\nby incorporating photometric features, as depicted in Figure 3. The parametric spec-\\nifications for the ViT remain consistent with those discussed in Section 4.2. Notably,\\nwe adjusted the dimension of the final dense layer, with d neurons, to 64, matching the\\ndimension of the flattened image patches. Table 4 illustrates the computational com-\\nplexity of the proposed MM ViT. Hybrid models that take both photometric features\\nand images (MargNet-based and MM ViT from Table 5) as input perform better than\\nindividual ANN (for photometric features, from Table 1) and CNN or ViT (for images,\\nfrom Table 2) models for both star-galaxy and star-galaxy-quasar classifications. The\\nperformance of hybrid models across all the experiments described in Section 2.1 are\\ndiscussed in detail in the sections below.\\nTable 4: Ensemble models with the num-\\nber of trainable parameters. The listed\\nnumbers are with three class (star-galaxy-\\nquasar) classification settings.\\nModel\\nNo. of Parameters\\nMargNet\\n25,875,217\\nSENet MargNet (r = 2)\\n26,626,769 (3 % ↑)\\nSENet MargNet (r = 8)\\n26,061,465 (0.7 % ↑)\\nSENet MargNet (r = 16)\\n25,967,041 (0.3 % ↑)\\nSENet MargNet (r = 32)\\n25,919,249 (0.1 % ↑)\\nViT MargNet\\n1,138,637 (96.6 % ↓)\\nMM ViT\\n1,505,315 (94.2 % ↓)\\n4.3.1 Results: Experiment 1\\nThis experiment selects all the three sets: training, validation, and testing from the\\nCompact Source dataset. Performance metrics are computed for star-galaxy and star-\\ngalaxy-quasar classifications as illustrated in Table 5. For star-galaxy classification,\\nquasars are excluded to facilitate comparison with only star-galaxy separation models\\nused in other studies. The proposed SENet-augmented MargNet achieves an overall\\naccuracy of 98.2 ± 0.1%, while MargNet, ViT-MargNet, and MM ViT each achieve an\\naverage accuracy of 98.1 ± 0.1%. We can also see from Figure 4 that the performance\\n16\\nTable 5: Quantitative performance of the MargNet and MM ViT models using photometric\\nfeatures and FITS images. While using MargNet, the pre-trained photometric features model\\n(ANN) is fixed and uses different image-based models, including CNN, SENet-augmented\\nCNN, and ViT.\\nExperiment\\nClasses\\nModel\\nAccuracy (%)\\nPrecision (%)\\nRecall (%)\\nExperiment 1\\n1. Star-Galaxy\\nMargNet\\n98.1 ± 0.1\\n98.1 ± 0.1\\n98.1 ± 0.1\\nSENet MargNet\\n98.2 ± 0.1\\n98.2 ± 0.1\\n98.2 ± 0.1\\nViT MargNet\\n98.1 ± 0.1\\n98.1 ± 0.1\\n98.1 ± 0.1\\nMM ViT\\n98.1 ± 0.1\\n98.1 ± 0.1\\n98.1 ± 0.1\\n2. Star-Galaxy-Quasar\\nMargNet\\n93.3 ± 0.2\\n93.3 ± 0.2\\n93.3 ± 0.2\\nSENet MargNet\\n93.5 ± 0.2\\n93.5 ± 0.2\\n93.5 ± 0.2\\nViT MargNet\\n93.2 ± 0.2\\n93.2 ± 0.2\\n93.2 ± 0.2\\nMM ViT\\n93.2 ± 0.2\\n93.2 ± 0.2\\n93.2 ± 0.2\\nExperiment 2\\n1. Star-Galaxy\\nMargNet\\n96.9 ± 0.1\\n96.9 ± 0.1\\n96.9 ± 0.1\\nSENet MargNet\\n97.1 ± 0.1\\n97.1 ± 0.1\\n97.1 ± 0.1\\nViT MargNet\\n96.8 ± 0.1\\n96.8 ± 0.1\\n96.8 ± 0.1\\nMM ViT\\n96.9 ± 0.1\\n96.9 ± 0.1\\n96.9 ± 0.1\\n2. Star-Galaxy-Quasar\\nMargNet\\n87.3 ± 0.2\\n87.4 ± 0.2\\n87.3 ± 0.2\\nSENet MargNet\\n87.5 ± 0.2\\n87.5 ± 0.2\\n87.5 ± 0.2\\nViT MargNet\\n86.9 ± 0.2\\n86.9 ± 0.2\\n86.9 ± 0.2\\nMM ViT\\n86.3 ± 0.2\\n86.2 ± 0.2\\n86.3 ± 0.2\\nExperiment 3\\n1. Star-Galaxy\\nMargNet\\n92.0 ± 0.1\\n92.8 ± 0.1\\n92.0 ± 0.1\\nSENet MargNet\\n92.9 ± 0.1\\n93.4 ± 0.1\\n92.9 ± 0.1\\nViT MargNet\\n91.7 ± 0.1\\n92.6 ± 0.1\\n91.7 ± 0.1\\nMM ViT\\n91.8 ± 0.1\\n92.5 ± 0.1\\n91.8 ± 0.1\\n2. Star-Galaxy-Quasar\\nMargNet\\n73.1 ± 0.2\\n76.4 ± 0.2\\n73.1 ± 0.2\\nSENet MargNet\\n73.2 ± 0.2\\n76.5 ± 0.2\\n73.2 ± 0.2\\nViT MargNet\\n72.7 ± 0.1\\n76.3 ± 0.1\\n72.7 ± 0.2\\nMM ViT\\n71.8 ± 0.2\\n75.4 ± 0.2\\n71.8 ± 0.2\\non both stars and galaxies is similar (98.31% for galaxies and 98.12% for stars) and\\nhigher with SENet-MargNet. MargNet exhibits a slightly higher accuracy for galaxies\\nat 93.32% while maintaining an accuracy of 98.01% for stars. Meanwhile, ViT-MargNet\\nand MM ViT demonstrate performance with an accuracy of 98.1% and 97.8% for\\ngalaxies 98.08% and 98.52% for stars, respectively. Notably, misclassifications between\\nstars and galaxies are minimal, with MargNet, SENet-MargNet, ViT-MargNet and\\nMM ViT achieving rates as low as approximately 1.83%, 1.78%, 1.91% and 1.84%,\\nrespectively.\\nFor the task of star-galaxy-quasar classification, the proposed SENet-MargNet\\nachieves an average accuracy of 93.5 ± 0.2%, with similar accuracies of 93.3 ± 0.2%,\\n93.2 ± 0.2% and 93.2 ± 0.2% observed for the MargNet, ViT-MargNet and MM\\nViT models, respectively. However, detailed analysis reveals variations in perfor-\\nmance across the individual classes. The confusion matrices in Figure 5 highlight\\ndistinct accuracies for stars, galaxies, and quasars. Quasars exhibit the lowest accu-\\nracy when classified by the ViT-MargNet model, achieving 90.63% accuracy, while\\nMM ViT achieves the best accuracy of 92.35%. An accuracy of 90.79% and 91.15%\\nare achieved using MargNet and SENet-MargNet, respectively. Notably, the MM ViT\\n17\\n(a) MargNet.\\n(b) SENet-MargNet.\\n(c) ViT-MargNet.\\n(d) MM ViT.\\nFig. 4: Confusion matrices of MargNet (a), SENet-augmented MargNet (b), ViT-\\nMargNet (c), and MM ViT (right) for Experiment 1, depicting star-galaxy classifi-\\ncation performance. The proposed SENet-MargNet outperforms all other models and\\nachieves 98.31% accuracy for galaxies and 98.12% for stars. Minimal misclassifications\\namong stars and galaxies are observed, with rates as low as approximately 1.78%,\\n1.83%, 1.91%, and 1.84% for SENet-MargNet, MargNet, ViT-MargNet, and MM ViT,\\nrespectively.\\nmodel demonstrates the lowest accuracy for stars, with a rate of 91.18%, and ViT-\\nMargNet demonstrates the lowest accuracy for galaxies at 95.63 %. Further analysis\\nreveals specific misclassifications within each model. For SENet-MargNet, quasars are\\nmisclassified as stars in approximately 5.85% of cases and galaxies in around 3%. The\\nMargNet model misidentifies quasars as stars in 6.13% of cases and galaxies in 3.08%.\\nThe ViT-MargNet misclassifies quasars as stars at 6.13% and galaxies at 3.23% of\\nrates. MM ViT misclassifies quasars as stars and galaxies at the same rates of 3.84%\\nand 3.81%, respectively. The more frequent identification of quasars as stars is under-\\nstandable, as both are point sources. For the same reason, stars are misclassified as\\n18\\n(a) MargNet.\\n(b) SENet-MargNet.\\n(c) ViT-MargNet.\\n(d) MM ViT.\\nFig. 5: Confusion matrices are presented for MargNet (a), SENet-augmented MargNet\\n(b), ViT-MargNet (c), and MM ViT (right) in Experiment 1, illustrating the classifica-\\ntion performance for star-galaxy-quasar categorization. The proposed SENet-MargNet\\nperforms better than all other models, achieving 95.89% accuracy for galaxies, 91.15%\\nfor quasars, and 93.51% for stars. Notably, minimal misclassifications are observed\\namong stars, quasars, and galaxies, with rates as low as approximately 3.24%, 3.26%,\\n3.41%, and 3.4% for SENet-MargNet, MargNet, ViT-MargNet, and MM ViT, respec-\\ntively.\\nquasars in ∼5.04%, 5.20%, 5.31% and 7.54% of the cases for the MargNet, SENet-\\nMargNet, ViT-MargNet and MM ViT, respectively. Galaxies have the best individual\\naccuracy among the three classes and across all the models. Galaxy-quasar misclassifi-\\ncations (galaxy-quasar: 2.90% and quasar-galaxy: 3.08% for MargNet, galaxy-quasar:\\n3.04% and quasar-galaxy: 3.0% for SENet-MargNet, galaxy-quasar: 3.13% and quasar-\\ngalaxy: 3.23% for ViT-MargNet and galaxy-quasar: 2.80% and quasar-galaxy: 3.81%\\nfor MM ViT being the individual misclassification rates) are more common than\\ngalaxy-star misclassifications (galaxy-star: 1.16% and star-galaxy: 1.26% for MargNet,\\n19\\ngalaxy-star: 1.07% and star-galaxy: 1.29% for SENet-MargNet, galaxy-star: 1.24% and\\nstar-galaxy: 1.39% for ViT-MargNet and galaxy-star: 1.13% and star-galaxy: 1.28%\\nfor MM ViT). This arises from the quasar host galaxy fuzz (Jahnke and Wisotzki,\\n2003), which may be observable particularly with low-luminosity quasars, sometimes\\nresulting in misclassification as a compact galaxy. Stars, devoid of structure beyond\\nthe Point Spread Function (PSF), are generally less susceptible to being misclassified\\nas galaxies.\\n(a) MargNet.\\n(b) SENet-MargNet.\\n(c) ViT-MargNet.\\n(d) MM ViT.\\nFig. 6: Confusion matrices are provided for MargNet (a), SENet-augmented MargNet\\n(b), ViT-MargNet (c), and MM ViT (d) in Experiment 2, focusing on star-galaxy\\nclassification performance. SENet-MargNet achieves high accuracy rates of 96.82% for\\nclassifying galaxies and 97.39% for stars. Importantly, misclassifications between stars\\nand galaxies are minimal across all models, with SENet-MargNet, MargNet, ViT-\\nMargNet, and MM ViT achieving rates as low as approximately 2.89%, 3.02%, 3.21%,\\nand 3.09%, respectively.\\n20\\n(a) MargNet.\\n(b) SENet-MargNet.\\n(c) ViT-MargNet.\\n(d) MM ViT.\\nFig. 7: Confusion matrices comparing MargNet (a), SENet-augmented MargNet (b),\\nViT-MargNet (c) and MM ViT (d) for Experiment 2 reveal their performance in\\nstar-galaxy-quasar classification. SENet-MargNet achieves 94.57% accuracy for galax-\\nies, 83.37% for quasars, and 84.8% for stars. Notably, misclassifications among stars,\\nquasars, and galaxies are minimal, with SENet-MargNet, MargNet, and MM ViT\\nachieving error rates as low as approximately 6.21%, 6.29%, 6.5% and 6.85%, respec-\\ntively.\\n4.3.2 Results: Experiment 2\\nIn Experiment 2, all datasets; training, validation, and test—are derived from the Faint\\nand Compact Source dataset. Similar to Experiment 1, various models are trained and\\nevaluated using the Experiment 2 dataset, and their performance metrics are sum-\\nmarized in Table 5. Across all metrics, there is a decline in performance compared to\\nExperiment 1, attributed to the lower signal-to-noise ratio (SNR) as objects become\\nfainter. This noise affects photometric measurements and feature calculations, pos-\\ning challenges in classification. For star-galaxy separation, SENet-MargNet achieves\\n97.1±0.1% accuracy, while MargNet, ViT-MargNet, and MM ViT achieve 96.9±0.1%,\\n21\\n(a) star–galaxy-quasar (SGQ) classification\\nperformance using MargNet for Experi-\\nment 2: We see accuracy sharply decreases\\ntill r = 21 and an unexpected rise for r >\\n22. For star recall, quasar recall, and quasar\\nprecision, a rise follows a drop can be seen.\\nAt r > 22, there is no rise seen in galaxy\\nprecision.\\n(b) star–galaxy-quasar (SGQ) classifica-\\ntion performance using SENet-MargNet for\\nExperiment 2: We see accuracy sharply\\ndecreases till r = 21 and an unexpected rise\\nfor r > 22. For star recall, quasar recall,\\nand quasar precision, a rise follows a drop\\ncan be seen. At r > 22, there is no rise seen\\nin galaxy precision.\\nFig. 8: In Experiment 2, performance on the test data is assessed for the faint and\\ncompact dataset (C < 0.5; r > 20), segmented into bins of 0.1 magnitude width,\\nwith the final range being 20 < r < 22.6. Precision, recall, and accuracy metrics are\\ncomputed for the star-galaxy-quasar classification problem, as depicted in Figure 8a\\nand Figure 8b for the MargNet and SENet-MargNet models, respectively. From the\\naccuracy plot, it is observed that the proposed SENet-MargNet is performing slightly\\nbetter than the baseline MargNet model.\\n22\\n(a) star–galaxy-quasar (SGQ) classification\\nperformance using ViT-MargNet for Exper-\\niment 2: We see accuracy sharply decreases\\ntill r = 21 and an unexpected rise for r >\\n22. For star recall, quasar recall, and quasar\\nprecision, a rise follows a drop can be seen.\\nAt r > 22, there is no rise seen in galaxy\\nprecision.\\n(b) star–galaxy-quasar (SGQ) classification\\nperformance using MM ViT for Experiment\\n2: We see accuracy sharply decreases till r\\n= 21 and an unexpected rise for r > 22. For\\nstar recall, quasar recall, and quasar preci-\\nsion, a rise follows a drop can be seen. At r\\n> 22, there is no rise seen in galaxy preci-\\nsion.\\nFig. 9: In Experiment 2, performance on the test data is assessed for the faint and\\ncompact dataset (C < 0.5; r > 20), segmented into bins of 0.1 magnitude width,\\nwith the final range being 20 < r < 22.6. Precision, recall, and accuracy metrics are\\ncomputed for the star-galaxy-quasar classification problem, as depicted in Figure 9a\\nand Figure 9b for the ViT-MargNet and MM ViT models, respectively. The ViT-\\nMargNet and MM ViT models perform very similarly to the baseline MargNet models.\\n23\\n96.8±0.1%, and 96.9±0.1%, respectively. Confusion matrices depicted in Figure 6\\nreveal SENet-MargNet’s excellence with 96.82% accuracy for galaxies and 97.39% for\\nstars. MargNet shows 96.76% accuracy for galaxies and 97.19% for stars. ViT-MargNet\\nand MM ViT score 96.72% and 96.78% for galaxies and 96.86% and 97.04% for stars,\\nrespectively. Minimal misclassifications between stars and galaxies are observed, with\\nrates of approximately 2.89%, 3.02%, 3.21%, and 3.09% for SENet-MargNet, MargNet,\\nViT-MargNet, and MM ViT, respectively.\\nSENet-MargNet achieves an average accuracy of 87.5±0.2% for star-galaxy-quasar\\nclassification, similar to MargNet, ViT-MargNet, and MM ViT, which achieve accura-\\ncies of 87.3±0.2%, 86.9±0.2%, and 86.3±0.2%, respectively. Individual class accuracies\\nare depicted in Figure 7. Quasars display the lowest accuracy with MM ViT at 81.28%\\nand the highest with SENet-MargNet at 83.37%. Stars achieve the lowest accuracy\\nwith MM ViT at 82.6% and the highest with MargNet at 85.1%. Similarly, for galax-\\nies, the lowest accuracy of 94.1% is achieved using ViT-MargNet and the highest with\\nMM ViT at 95.01%. Notably, misclassifications among stars, quasars, and galaxies are\\nminimal, with SENet-MargNet, MargNet, and MM ViT achieving error rates as low as\\napproximately 6.21%, 6.29%, 6.5%, and 6.85%, respectively. We further evaluate the\\nperformance statistics as a function of magnitude to assess model behaviour at fainter\\nlevels. The test set is divided into bins of 0.1 magnitudes within the 20 < r < 22.6\\nrange, ensuring each bin contains at least 50 objects per class for robust analy-\\nsis. Metrics are assessed for each bin and visualized in Figure 8 and Figure 9. For\\nstar-galaxy-quasar classification, Figure 8a (MargNet model) shows accuracy steadily\\ndecreasing up to r = 21.3, in line with diminishing SNR.\\nAcross all the models, the accuracy starts increasing beyond r > 21.3 for both\\nstar-galaxy and star-galaxy-quasar classifications. This is unexpected, as fainter galax-\\nies are usually harder to classify and thus should lower the accuracy. Similar trends\\nwere observed in C23. The discrepancy might stem from the artifact in the training\\ndataset distribution across different r magnitudes, containing 2831 galaxies, 190 stars,\\nand 660 quasars for r > 21.3, despite equal class representation overall. On the other\\nhand, the testing dataset maintains an equal class distribution across magnitude bins\\nto avoid bias. This training bias reduces false negatives for galaxies, leading to fewer\\nfalse positives for stars and quasars, thus potentially boosting the precision at fainter\\nmagnitudes. Similar trends are observed for quasar precision, quasar recall, and star\\nrecall, with a rise following a drop. Star precision and galaxy recall remain consis-\\ntently high. However, galaxy precision behaves differently; at r > 22, no increase is\\nobserved, plateauing. A similar trend is observed for SENet-MargNet (in Figure 8b),\\nViT-Margnet (in Figure 9a) and MM ViT (in Figure 9b) models.\\n4.3.3 Results: Experiment 3\\nExperiment 3 selects the training and validation sets from the Compact Source dataset,\\nwhile the test set is drawn from the Faint and Compact Source dataset. As in Exper-\\niments 1 and 2, a range of models are trained and evaluated using the Experiment 3\\ntest dataset, and their performance metrics are summarized in Table 5. Given that\\nthe training data remains consistent between Experiments 1 and 3, we directly employ\\nthe models trained on Experiment 1’s training data to assess their performance on\\n24\\n(a) MargNet.\\n(b) SENet-MargNet.\\n(c) ViT-MargNet.\\n(d) MM ViT.\\nFig. 10: Confusion matrices are provided for MargNet (a), SENet-augmented\\nMargNet (b), ViT-MargNet (c), and MM ViT (d) in Experiment 3, focusing on\\nstar-galaxy classification performance. SENet-MargNet achieves high accuracy rates\\nof 88.48% for classifying galaxies and 98.43% for stars. Importantly, misclassifica-\\ntions between stars and galaxies are minimal across all models, with SENet-MargNet,\\nMargNet, ViT-MargNet, and MM ViT achieving rates as low as approximately 6.54%,\\n6.85%, 7.39%, and 7.41%, respectively.\\nthe Experiment 3 test data. However, across all metrics, a decline in performance is\\nevident compared to Experiment 1, attributed to the lower SNR as objects become\\nfainter. For star-galaxy separation, SENet-MargNet achieves an accuracy of 92.9 ±\\n0.1%, while MargNet, ViT-MargNet, and MM ViT achieve 92.0±0.1%, 91.7±0.1%,\\nand 91.8±0.1% accuracy, respectively. The confusion matrices illustrated in Figure 10\\nhighlight SENet-MargNet’s excellence with an accuracy of 88.48% for galaxies and\\n98.43% for stars, followed closely by MargNet with accuracies of 87.67% for galaxies\\nand 98.62% for stars. ViT-MargNet and MM ViT achieve accuracies of 86.75% and\\n86.81% for galaxies and 98.46% and 98.37% for stars, respectively. Notably, minimal\\n25\\n(a) MargNet.\\n(b) SENet-MargNet.\\n(c) ViT-MargNet.\\n(d) MM ViT.\\nFig. 11: Confusion matrices comparing MargNet (a), SENet-augmented MargNet (b),\\nViT-MargNet (c) and MM ViT (d) for Experiment 3 reveal their performance in star-\\ngalaxy-quasar classification. SENet-MargNet achieves 65.24% accuracy for galaxies,\\n69.04% for quasars, and 95.14% for stars. Notably, misclassifications among stars,\\nquasars, and galaxies are minimal, with SENet-MargNet, MargNet, ViT-MargNet and\\nMM ViT achieving error rates as low as approximately 11.76%, 11.87%, 11.81%, and\\n12.28% respectively.\\nmisclassifications between stars and galaxies are observed, with rates of approximately\\n6.54%, 6.85%, 7.39%, and 7.41% for SENet-MargNet, MargNet, ViT-MargNet, and\\nMM ViT, respectively.\\nSimilarly, for the star-galaxy-quasar classification, SENet-MargNet achieves an\\naverage accuracy of 73.2 ± 0.2%, on par with MargNet, ViT-MargNet, and MM\\nViT, which achieve accuracies of 73.1±0.2%, 72.7±0.2%, and 71.8±0.2%, respec-\\ntively. Figure 11 illustrates the individual class accuracies. Quasars exhibit the lowest\\naccuracy, ranging from 66.2% with MM ViT to 69.04% with SENet-MargNet. For\\nstars, SENet-MargNet achieves the lowest accuracy at 95.14%, while ViT-MargNet\\n26\\nachieves the highest at 95.5%. Similarly, in galaxies, the lowest accuracy of 64.72%\\nis attained using MM ViT, while SENet-MargNet achieves the highest at 65.24%.\\nNotably, misclassifications among stars, quasars, and galaxies remain minimal, with\\nSENet-MargNet, MargNet, and MM ViT demonstrating error rates as low as approxi-\\nmately 11.76%, 11.87%, 11.81%, and 12.28%, respectively. Across all experiments and\\nclassification settings, the proposed SENet-MargNet consistently outperforms all other\\nmodels. While the performance enhancement achieved with SENet augmentation on\\nMargNet incurs an additional computational cost of approximately 0.1% to 3% in\\ntrainable parameters, integrating ViT into MargNet and MM ViT requires only 3.4%\\nand 5.8% of the parameters of the baseline MargNet model, respectively. Notably, MM\\nViT performs comparably to ViT-MargNet.\\n5 Conclusions and Future Work\\nOur study presents the results aimed at enhancing the performance of the previously\\nintroduced MargNet model, utilizing attention mechanism and ViT-based architec-\\ntures for classifying stars, galaxies, and quasars in SDSS photometric images. Within\\nthe SDSS dataset, our proposed MargNet variants, particularly SENet-MargNet,\\nexhibit superior classification accuracy compared to prior neural network-based\\napproaches, particularly for compact and faint galaxies. Additionally, our ViT-\\nMargNet and MM ViT models offer lightweight alternatives. A significant advantage\\nof CNN and ViT-based models is their ability to learn useful features directly from\\nimages automatically. This alleviates the need for separate feature engineering, as\\ntypically required in traditional machine learning algorithms. Notably, a portion of\\nMargNet’s architecture incorporates inception net modules, which have demonstrated\\npromising results in various image classification tasks and have gained widespread\\nadoption within the computer vision community. While conventional networks tend\\nto exhibit reduced accuracy as sources become fainter and more compact, SENet-\\nMargNet showcases the promising performance, achieving an overall accuracy of\\n93.5%, with MargNet achieving 93.5%, and ViT-MargNet and MM ViT models achiev-\\ning accuracies of 93.2% each. When considering faint and compact sources, individual\\nCNN and ANN components achieve approximately 91.6% and 93.0% accuracy, respec-\\ntively. However, notable improvement in accuracy is observed when these components\\nare combined within MargNet and MM ViT. The applicability of MargNet-based\\nmodels can be extended to future surveys, given their ability to effectively capture\\nnumerous faint and compact sources. Furthermore, MargNet holds promise for sur-\\nveys like GAIA and ZTF, which rely on crossmatching with SDSS and employ transfer\\nlearning techniques. Even in scenarios where photometric features differ from SDSS,\\nadjustments to the ANN network within MargNet can accommodate alternative fea-\\ntures. Alternatively, the image-based CNN component of MargNet can be utilized\\nindependently if photometric features are unavailable. In conclusion, the methodology\\nunderlying MargNet proves valuable due to its high accuracy in object classification,\\nmaking it well-suited for deployment in forthcoming astronomical surveys with deeper\\nobservational reach.\\n27\\n6 Acknowledgements\\nSrinadh Reddy was supported by Tata Consultancy Services (TCS) and the Depart-\\nment of Science and Technology - Interdisciplinary Cyber-Physical Systems (DST-\\nICPS) (under the grant T-641). He is grateful to Ajit Kembhavi and Yogesh\\nWadadekar for useful feedback on his thesis defense presentation. We thank the Sloan\\nDigital Sky Survey for making their data free and open source. The Alfred P. Sloan\\nFoundation, the U.S. Department of Energy Office of Science, and the Participating\\nInstitutions have funded SDSS IV. SDSS-IV acknowledges support and resources from\\nthe Center for High-Performance Computing at the University of Utah. The SDSS\\nwebsite is www.sdss.org. SDSS-IV is managed by the Astrophysical Research Con-\\nsortium for the Participating Institutions of the SDSS Collaboration including the\\nBrazilian Participation Group, the Carnegie Institution for Science, Carnegie Mellon\\nUniversity, Center for Astrophysics — Harvard & Smithsonian, the Chilean Partici-\\npation Group, the French Participa- tion Group, Instituto de Astrof´ısica de Canarias,\\nThe Johns Hopkins University, Kavli Institute for the Physics and Mathematics\\nof the Uni- verse (IPMU) / University of Tokyo, the Korean Participation Group,\\nLawrence Berkeley National Laboratory, Leibniz Institut f¨ur Astro- physik Potsdam\\n(AIP), Max-Planck-Institut f¨ur Astronomie (MPIA Heidelberg), Max-Planck-Institut\\nf¨ur Astrophysik (MPA Garching), Max-Planck-Institut f¨ur Extraterrestrische Physik\\n(MPE), National Astronomical Observatories of China, New Mexico State Univer- sity,\\nNew York University, University of Notre Dame, Observat´ario Nacional / MCTI, The\\nOhio State University, Pennsylvania State University, Shanghai Astronomical Obser-\\nvatory, United Kingdom Participation Group, Universidad Nacional Aut´onoma de\\nM´exico, University of Arizona, University of Colorado Boulder, University of Oxford,\\nUniversity of Portsmouth, University of Utah, Univer- sity of Virginia, University of\\nWashington, University of Wisconsin, Vanderbilt University, and Yale University.\\n7 Software\\nAstropy (Robitaille et al, 2013), Numpy (Harris et al, 2020), Scipy (Virtanen et al,\\n2020), Matplotlib (Hunter, 2007), Seaborn (Waskom, 2021), Pandas (Reback et al,\\n2022), Jupyter (Kluyver et al, 2016), scikit-learn (Pedregosa et al, 2011), Python3\\n(Van Rossum GaDJ, 2009), and Pytorch (Paszke et al, 2019).\\nAppendix A\\nModel Training Details\\nWe have implemented our deep-learning models in PyTorch 1.9.0 using the Adam\\noptimizer (Kingma and Ba, 2014). The CNN, SENet-CNN, and ViT (for images) and\\nANN (for photometric features) models underwent training with an early stopping\\ncriterion, ceasing training if there was no improvement over 30 and 100 consecutive\\nepochs, respectively. Finally, the ensemble models (MargNet, SENet-MargNet and\\nViT-MargNet) were trained for 100 epochs for the last layers freezing the correspond-\\ning photometric feature-based and image-based models, with the best-performing\\nmodels being saved. For the MM ViT, an early stopping criterion was applied, ceasing\\ntraining when there was no further improvement over 100 consecutive epochs. Each\\n28\\nmodel (ANN, CNN and ViT-based) was trained five times during training, and the\\naverage results are presented.\\nAppendix B\\nData Availability\\nThe PyTorch implementation of the proposed framework has been made open-source\\nand is publicly available on GitHub. Photometric data was acquired from SDSS\\nCasjobs DR16 using a specific query, while image data was obtained from SDSS\\nthrough a Python script. The fully preprocessed dataset is accessible on Zenodo.\\nReferences\\nAbraham S, Aniyan A, Kembhavi AK, et al (2018) Detection of bars in galaxies using\\na deep convolutional neural network. Monthly Notices of the Royal Astronomical\\nSociety 477(1):894–903\\nAhumada R, Prieto CA, Almeida A, et al (2020) The 16th data release of the sloan\\ndigital sky surveys: first release from the apogee-2 southern survey and full release\\nof eboss spectra. The Astrophysical Journal Supplement Series 249(1):3\\nBall NM, Brunner RJ (2010) Data mining and machine learning in astronomy.\\nInternational Journal of Modern Physics D 19(07):1049–1106\\nBarchi PH, de Carvalho R, Rosa RR, et al (2020) Machine and deep learning applied\\nto galaxy morphology-a comparative study. Astronomy and Computing 30:100334\\nBaron D (2019) Machine learning in astronomy: A practical overview. arXiv preprint\\narXiv:190407248\\nBazi Y, Bashmal L, Rahhal MMA, et al (2021) Vision transformers for remote sensing\\nimage classification. Remote Sensing 13(3):516\\nBellm E (2014) The zwicky transient facility. In: The Third Hot-wiring the Transient\\nUniverse Workshop\\nBethapudi S, Desai S (2018) Separation of pulsar signals from noise using supervised\\nmachine learning algorithms. Astronomy and Computing 23:15. arXiv:1704.04659\\n[astro-ph.IM]\\nBhavanam SR, Channappayya SS, Srijith P, et al (2022a) Cosmic ray detection in\\nastronomical images via dictionary learning and sparse representation. In: 2022 30th\\nEuropean Signal Processing Conference (EUSIPCO), IEEE, pp 1966–1970\\nBhavanam SR, Channappayya SS, Srijith P, et al (2022b) Cosmic ray rejection with\\nattention augmented deep learning. Astronomy and Computing 40:100625\\nBreiman L (2001) Random forests. Machine learning 45:5–32\\nCabayol L, Sevilla-Noarbe I, Fern´andez E, et al (2019) The pau survey: star–\\ngalaxy classification with multi narrow-band data. Monthly Notices of the Royal\\nAstronomical Society 483(1):529–539\\nChaini S, Bagul A, Deshpande A, et al (2023) Photometric identification of compact\\ngalaxies, stars, and quasars using multiple neural networks. MNRAS518(2):3123–\\n3136. arXiv:2211.08388 [astro-ph.GA]\\nChang C, Drlica-Wagner A, Kent SM, et al (2021) A machine learning approach to\\nthe detection of ghosting and scattered light artifacts in dark energy survey images.\\nAstronomy and Computing 36:100474\\n29\\nChen J, Lu Y, Yu Q, et al (2021) Transunet: Transformers make strong encoders for\\nmedical image segmentation. arXiv preprint arXiv:210204306\\nChen T, Guestrin C (2016) Xgboost: A scalable tree boosting system. In: Proceedings\\nof the 22nd acm sigkdd international conference on knowledge discovery and data\\nmining, pp 785–794\\nCheng TY, Li N, Conselice CJ, et al (2020) Identifying strong lenses with unsupervised\\nmachine learning using convolutional autoencoder. Monthly Notices of the Royal\\nAstronomical Society 494(3):3750–3765\\nDark Energy Survey Collaboration, Abbott T, Abdalla FB, et al (2016) The Dark\\nEnergy Survey: more than dark energy - an overview. MNRAS460(2):1270–1299.\\narXiv:1601.00329 [astro-ph.CO]\\nDesai S, Armstrong R, Mohr JJ, et al (2012) The Blanco Cosmology Survey:\\nData Acquisition, Processing, Calibration, Quality Diagnostics, and Data Release.\\nApJ757(1):83. arXiv:1204.1210 [astro-ph.CO]\\nDosovitskiy A, Beyer L, Kolesnikov A, et al (2020) An image is worth 16x16 words:\\nTransformers for image recognition at scale. arXiv preprint arXiv:201011929\\nEuclid Collaboration, Scaramella R, Amiaux J, et al (2022) Euclid preparation. I.\\nThe Euclid Wide Survey. Astronomy and Astrophysics662:A112. https://doi.org/\\n10.1051/0004-6361/202141938, arXiv:2108.01201 [astro-ph.CO]\\nFadely R, Hogg DW, Willman B (2012) Star–galaxy classification in multi-band optical\\nimaging. The Astrophysical Journal 760(1):15\\nFukugita M, Shimasaku K, Ichikawa T, et al (1996) The sloan digital sky survey\\nphotometric system. Tech. rep., SCAN-9601313\\nGeorge D, Huerta EA (2018) Deep learning for real-time gravitational wave detec-\\ntion and parameter estimation: Results with advanced ligo data. Physics Letters B\\n778:64–70\\nGuo MH, Xu TX, Liu JJ, et al (2022) Attention mechanisms in computer vision: A\\nsurvey. Computational Visual Media 8(3):331–368\\nGupta R, Srijith P, Desai S (2022) Galaxy morphology classification using neural\\nordinary differential equations. Astronomy and Computing 38:100543\\nHao-ran Q, Ji-ming L, Jun-yi W (2017) Stacked denoising autoencoders applied to\\nstar/galaxy classification. Chinese Astronomy and Astrophysics 41(2):282–292\\nHarris CR, Millman KJ, van der Walt SJ, et al (2020) Array programming with numpy.\\nNature 585(7825):357–362\\nHe K, Gan C, Li Z, et al (2023) Transformers in medical image analysis. Intelligent\\nMedicine 3(1):59–78\\nHu J, Shen L, Sun G (2018) Squeeze-and-excitation networks. In: Proceedings of the\\nIEEE conference on computer vision and pattern recognition, pp 7132–7141\\nHunter JD (2007) Matplotlib: A 2d graphics environment. Computing in science &\\nengineering 9(03):90–95\\nIvezi´c ˇZ, Kahn SM, Tyson JA, et al (2019) Lsst: from science drivers to reference\\ndesign and anticipated data products. The Astrophysical Journal 873(2):111\\nJahnke K, Wisotzki L (2003) The b-band luminosities of quasar host galaxies. Monthly\\nNotices of the Royal Astronomical Society 346(1):304–318\\n30\\nKarpoor P (2022) Morphological Classification of Galaxies using Vision Transformer\\nModels. In: American Astronomical Society Meeting #240, p 201.13\\nKim EJ, Brunner RJ (2017) Star-galaxy classification using deep convolutional neu-\\nral networks. MNRAS464(4):4463–4475. https://doi.org/10.1093/mnras/stw2672,\\narXiv:1608.04369 [astro-ph.IM]\\nKingma DP, Ba J (2014) Adam: A method for stochastic optimization. arXiv preprint\\narXiv:14126980\\nKluyver T, Ragan-Kelley B, P´erez F, et al (2016) Jupyter notebooks-a publishing\\nformat for reproducible computational workflows. Elpub 2016:87–90\\nKrizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classification with deep con-\\nvolutional neural networks. Advances in neural information processing systems\\n25\\nKuntzer T, Tewes M, Courbin F (2016) Stellar classification from single-band imaging\\nusing machine learning. Astronomy & Astrophysics 591:A54\\nLahav O (2023) Deep Machine Learning in Cosmology: Evolution or Revolu-\\ntion? arXiv e-prints arXiv:2302.04324. https://doi.org/10.48550/arXiv.2302.04324,\\narXiv:2302.04324 [astro-ph.CO]\\nLi Y, Mao H, Girshick R, et al (2022) Exploring plain vision transformer backbones\\nfor object detection. In: European Conference on Computer Vision, Springer, pp\\n280–296\\nLin JYY, Liao SM, Huang HJ, et al (2021) Galaxy morphological classification with\\nefficient vision transformer. arXiv preprint arXiv:211001024\\nLochner M, McEwen JD, Peiris HV, et al (2016) Photometric supernova classification\\nwith machine learning. The Astrophysical Journal Supplement Series 225(2):31\\nL´opez-Sanjuan C, Rami´o HV, Varela J, et al (2019) J-plus: Morphological star/galaxy\\nclassification by pdf analysis. Astronomy & Astrophysics 622:A177\\nMahabal A, Rebbapragada U, Walters R, et al (2019) Machine learning for the\\nzwicky transient facility. Publications of the Astronomical Society of the Pacific\\n131(997):038002\\nMiyazaki\\nS,\\nKomiyama\\nY,\\nSekiguchi\\nM,\\net\\nal\\n(2002)\\nSubaru\\nprime\\nfocus\\ncamera—suprime-cam.\\nPublications\\nof\\nthe\\nAstronomical\\nSociety\\nof\\nJapan\\n54(6):833–853\\nM¨oller A, de Boissi`ere T (2020) Supernnova: an open-source framework for bayesian,\\nneural network-based supernova classification. Monthly Notices of the Royal Astro-\\nnomical Society 491(3):4277–4293\\nNakazono L, Mendes de Oliveira C, Hirata NST, et al (2021) On the discovery of stars,\\nquasars, and galaxies in the southern hemisphere with s-plus dr2. Monthly Notices\\nof the Royal Astronomical Society 507(4):5847–5868\\nOdewahn SC, Stockwell EB, Pennington RL, et al (1992) Automated Star/Galaxy\\nDiscrimination With Neural Networks. AJ103:318. https://doi.org/10.1086/116063\\nPasquet J, Bertin E, Treyer M, et al (2019) Photometric redshifts from sdss images\\nusing a convolutional neural network. Astronomy & Astrophysics 621:A26\\nPaszke A, Gross S, Massa F, et al (2019) Pytorch: An imperative style, high-\\nperformance deep learning library. Advances in neural information processing\\nsystems 32:8026–8037\\n31\\nPedregosa F, Varoquaux G, Gramfort A, et al (2011) Scikit-learn: Machine learning\\nin python. the Journal of machine Learning research 12:2825–2830\\nQuinlan JR (1986) Induction of decision trees. Machine learning 1:81–106\\nReback J, Jbrockmendel MW, Van Den Bossche J, et al (2022) pandas-dev/pandas:\\nPandas 1.4. 4. zenodo\\nRobitaille TP, Tollerud EJ, Greenfield P, et al (2013) Astropy: A community python\\npackage for astronomy. Astronomy & Astrophysics 558:A33\\nRoy SK, Deria A, Hong D, et al (2023) Multimodal fusion transformer for remote\\nsensing image classification. IEEE Transactions on Geoscience and Remote Sensing\\nSebok WL (1986) The angular correlation function of galaxies as a function of magni-\\ntude. Astrophysical Journal Supplement Series (ISSN 0067-0049), vol 62, Oct 1986,\\np 301-330 62:301–330\\nSharma K, Kembhavi A, Kembhavi A, et al (2020a) Application of convolutional\\nneural networks for stellar spectral classification. Monthly Notices of the Royal\\nAstronomical Society 491(2):2280–2300\\nSharma K, Singh HP, Gupta R, et al (2020b) Stellar spectral interpolation using\\nmachine learning. Monthly Notices of the Royal Astronomical Society 496(4):5002–\\n5016\\nSimonyan K, Zisserman A (2014) Very deep convolutional networks for large-scale\\nimage recognition. arXiv preprint arXiv:14091556\\nSlater CT, Ivezi´c ˇZ, Lupton RH (2020) Morphological star–galaxy separation. The\\nAstronomical Journal 159(2):65\\nSoumagnac MT, Abdalla FB, Lahav O, et al (2015) Star/galaxy separation at faint\\nmagnitudes: application to a simulated Dark Energy Survey. MNRAS450(1):666–\\n680. arXiv:1306.5236 [astro-ph.CO]\\nSrivastava N, Hinton G, Krizhevsky A, et al (2014) Dropout: a simple way to pre-\\nvent neural networks from overfitting. The journal of machine learning research\\n15(1):1929–1958\\nSzegedy C, Liu W, Jia Y, et al (2015) Going deeper with convolutions. In: Proceedings\\nof the IEEE conference on computer vision and pattern recognition, pp 1–9\\nTanoglidis D, ´Ciprijanovi´c A, Drlica-Wagner A, et al (2022) Deepghostbusters: Using\\nmask r-cnn to detect and mask ghosting and scattered-light artifacts from optical\\nsurvey images. Astronomy and Computing 39:100580\\nVan Rossum GaDJ FL (2009) Python 3 reference manual. Scotts Valley, CA:\\nCreateSpace\\nVasconcellos E, De Carvalho R, Gal R, et al (2011) Decision tree classifiers for\\nstar/galaxy separation. The Astronomical Journal 141(6):189\\nVaswani A, Shazeer N, Parmar N, et al (2017) Attention is all you need. Advances in\\nneural information processing systems 30\\nVirtanen P, Gommers R, Oliphant TE, et al (2020) Scipy 1.0: fundamental algorithms\\nfor scientific computing in python. Nature methods 17(3):261–272\\nWang C, Bai Y, L´opez-Sanjuan C, et al (2022) J-plus: Support vector machine applied\\nto star-galaxy-qso classification. Astronomy & Astrophysics 659:A144\\nWang S, Li BZ, Khabsa M, et al (2020) Linformer: Self-attention with linear\\ncomplexity. arXiv preprint arXiv:200604768\\n32\\nWaskom ML (2021) Seaborn: statistical data visualization. Journal of Open Source\\nSoftware 6(60):3021\\nWen XQ, Yang JM (2021) Classification of star/galaxy/QSO and star spectral types\\nfrom LAMOST data release 5 with machine learning approaches. Chinese Journal\\nof Physics 69:303–311\\nWoo S, Park J, Lee JY, et al (2018) Cbam: Convolutional block attention module. In:\\nProceedings of the European conference on computer vision (ECCV), pp 3–19\\nXu C, McCully C, Dong B, et al (2023) Cosmic-conn: A cosmic-ray detection deep-\\nlearning framework, data set, and toolkit. The Astrophysical Journal 942(2):73\\nYang L, Zhang RY, Li L, et al (2021) Simam: A simple, parameter-free attention\\nmodule for convolutional neural networks. In: International conference on machine\\nlearning, PMLR, pp 11863–11874\\nYork DG, Adelman J, Anderson Jr JE, et al (2000) The sloan digital sky survey:\\nTechnical summary. The Astronomical Journal 120(3):1579\\nZhang K, Bloom JS (2020) deepcr: Cosmic ray rejection with deep learning. The\\nAstrophysical Journal 889(1):24\\nZhang Y (2022) Classification of quasars, galaxies, and stars by using xgboost in\\nsdss-dr16. In: 2022 International Conference on Machine Learning and Knowledge\\nEngineering (MLKE), IEEE, pp 266–272\\n33\\n')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.wikipedia.WikipediaLoader at 0x1861dddda00>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "loader = WikipediaLoader(query=\"India\", lang=\"en\", doc_content_chars_max=25)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'India', 'summary': \"India, officially the Republic of India (ISO: Bhārat Gaṇarājya), is a country in South Asia.  It is the seventh-largest country by area; the most populous country from June 2023 and from the time of its independence in 1947, the world's most populous democracy. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar, and Indonesia.\\nModern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago.\\nTheir long occupation, initially in varying forms of isolation as hunter-gatherers, has made the region highly diverse, second only to Africa in human genetic diversity. Settled life emerged on the subcontinent in the western margins of the Indus river basin 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BCE.\\nBy 1200 BCE, an archaic form of Sanskrit, an Indo-European language, had diffused into India from the northwest. Its evidence today is found in the hymns of the Rigveda. Preserved by an oral tradition that was resolutely vigilant, the Rigveda records the dawning of Hinduism in India. The Dravidian languages of India were supplanted in the northern and western regions. By 400 BCE, stratification and exclusion by caste had emerged within Hinduism, and Buddhism and Jainism had arisen, proclaiming social orders unlinked to heredity.\\nEarly political consolidations gave rise to the loose-knit Maurya and Gupta Empires based in the Ganges Basin.\\nTheir collective era was suffused with wide-ranging creativity, but also marked by the declining status of women, and the incorporation of untouchability into an organised system of belief. In South India, the Middle kingdoms exported Dravidian-languages scripts and religious cultures to the kingdoms of Southeast Asia.\\nIn the early medieval era, Christianity, Islam, Judaism, and Zoroastrianism became established on India's southern and western coasts.\\nMuslim armies from Central Asia intermittently overran India's northern plains,\\neventually founding the Delhi Sultanate, and drawing northern India into the cosmopolitan networks of medieval Islam.\\nIn the 15th century, the Vijayanagara Empire created a long-lasting composite Hindu culture in south India.\\nIn the Punjab, Sikhism emerged, rejecting institutionalised religion.\\nThe Mughal Empire, in 1526, ushered in two centuries of relative peace,\\nleaving a legacy of luminous architecture.\\nGradually expanding rule of the British East India Company followed, turning India into a colonial economy, but also consolidating its sovereignty. British Crown rule began in 1858. The rights promised to Indians were granted slowly, but technological changes were introduced, and modern ideas of education and the public life took root. A pioneering and influential nationalist movement emerged, which was noted for nonviolent resistance and became the major factor in ending British rule. In 1947 the British Indian Empire was partitioned into two independent dominions, a Hindu-majority Dominion of India and a Muslim-majority Dominion of Pakistan, amid large-scale loss of life and an unprecedented migration.\\nIndia has been a federal republic since 1950, governed through a democratic parliamentary system. It is a pluralistic, multilingual and multi-ethnic society. India's population grew from 361 million in 1951 to almost 1.4 billion in 2022.\\nDuring the same time, its nominal per capita income increased from US$64 annually to US$2,601, and its literacy rate from 16.6% to 74%. From being a comparatively destitute country in 1951, India has become a fast-growing major economy and a hub for information technology services, with an expanding middle class. India has a space programme with several planned or completed extraterrestrial missions. Indian movies, music, and spiritual teachings play an increasing role in global culture. India has substantially reduced its rate of poverty, though at the cost of increasing economic inequality. India is a nuclear-weapon state, which ranks high in military expenditure. It has disputes over Kashmir with its neighbours, Pakistan and China, unresolved since the mid-20th century. Among the socio-economic challenges India faces are gender inequality, child malnutrition, and rising levels of air pollution. India's land is megadiverse, with four biodiversity hotspots. Its forest cover comprises 21.7% of its area. India's wildlife, which has traditionally been viewed with tolerance in India's culture, is supported among these forests, and elsewhere, in protected habitats.\", 'source': 'https://en.wikipedia.org/wiki/India'}, page_content='India, officially the Rep'),\n",
       " Document(metadata={'title': 'Indian National Developmental Inclusive Alliance', 'summary': \"The Indian National Developmental Inclusive Alliance (I.N.D.I.A.) is a big tent multi party political alliance of several political parties in India led by the country's largest opposition party, the Indian National Congress. The alliance is in opposition to the ruling National Democratic Alliance (NDA) government led by the Bharatiya Janata Party (BJP) in the 2024 Indian general elections. In the 2024 general election, the alliance won 234 seats, gaining more than 100 seats from dissolution, and the majority of seats in states like Uttar Pradesh, Maharashtra, and West Bengal. The BJP lost its sole majority, with the alliance forcing Modi to govern with his coalition, the NDA.\", 'source': 'https://en.wikipedia.org/wiki/Indian_National_Developmental_Inclusive_Alliance'}, page_content='The Indian National Devel'),\n",
       " Document(metadata={'title': 'Punjab, India', 'summary': \"Punjab ( ; Punjabi: [pənˈdʒɑːb] ) is a state in northwestern India. Forming part of the larger Punjab region of the Indian subcontinent, the state is bordered by the Indian states of Himachal Pradesh to the north and northeast, Haryana to the south and southeast, and Rajasthan to the southwest; by the Indian union territories of Chandigarh to the east and Jammu and Kashmir to the north. It shares an international border with Punjab, a province of Pakistan to the west. The state covers an area of 50,362 square kilometres (19,445 square miles), which is 1.53% of India's total geographical area, making it the 19th-largest Indian state by area out of 28 Indian states (20th largest, if Union Territories are considered). With over 27 million inhabitants, Punjab is the 16th-largest Indian state by population, comprising 23 districts. Punjabi, written in the Gurmukhi script, is the most widely spoken and the official language of the state. The main ethnic group are the Punjabis, with Sikhs (57.7%) and Hindus (38.5%) forming the dominant religious groups. The state capital, Chandigarh, is a union territory and also the capital of the neighbouring state of Haryana. Three tributaries of the Indus River — the Sutlej, Beas, and Ravi — flow through Punjab.\\nThe history of Punjab has witnessed the migration and settlement of different tribes of people with different cultures and ideas, forming a civilisational melting pot. The ancient Indus Valley Civilisation flourished in the region until its decline around 1900 BCE. Punjab was enriched during the height of the Vedic period, but declined in predominance with the rise of the Mahajanapadas. The region formed the frontier of initial empires during antiquity including Alexander's and the Maurya empires. It was subsequently conquered by the Kushan Empire, Gupta Empire, and then Harsha's Empire. Punjab continued to be settled by nomadic people; including the Huna, Turkic and the Mongols. Punjab came under Muslim rule c.\\u20091000 CE, and was part of the Delhi Sultanate and the Mughal Empire. Sikhism, based on the teachings of Sikh Gurus, emerged between the 15th and 17th centuries. Conflicts between the Mughals and the later Sikh Gurus precipitated a militarisation of the Sikhs, resulting in the formation of a confederacy after the weakening of the Mughal Empire, which competed for control with the larger Durrani Empire. This confederacy was united in 1801 by Maharaja Ranjit Singh, forming the Sikh Empire.\\nThe larger Punjab region was annexed by the British East India Company from the Sikh Empire in 1849. At the time of the independence of India from British rule in 1947, the Punjab province was partitioned along religious lines amidst widespread violence, with the Muslim-majority western portion becoming part of Pakistan and the Hindu- and Sikh-majority east remaining in India, causing a large-scale migration between the two. After the Punjabi Suba movement, Indian Punjab was reorganised on the basis of language in 1966, when its Haryanvi- and Hindi-speaking areas were carved out as Haryana, Pahari-speaking regions attached to Himachal Pradesh and the remaining, mostly Punjabi-speaking areas became the current state of Punjab. A separatist insurgency occurred in the state during the 1980s. At present, the economy of Punjab is the 15th-largest state economy in India with ₹8.02 trillion (equivalent to ₹8.0 trillion or US$96 billion in 2023) in gross domestic product and a per capita GDP of ₹264,000 (equivalent to ₹260,000 or US$3,200 in 2023), ranking 17th among Indian states. Since independence, Punjab is predominantly an agrarian society. It is the ninth-highest ranking among Indian states in human development index. Punjab has bustling tourism, music, culinary, and film industries.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Punjab,_India'}, page_content='Punjab ( ; Punjabi: [pənˈ'),\n",
       " Document(metadata={'title': 'East India Company', 'summary': 'The East India Company (EIC) was an English, and later British, joint-stock company founded in 1600 and dissolved in 1874. It was formed to trade in the Indian Ocean region, initially with the East Indies (South Asia and Southeast Asia), and later with East Asia. The company gained control of large parts of South Asia and Hong Kong. At its peak, the company was the largest corporation in the world by various measures and had its own armed forces in the form of the company\\'s three presidency armies, totalling about 260,000 soldiers, twice the size of the British Army at certain times.\\nOriginally chartered as the \"Governor and Company of Merchants of London Trading into the East-Indies\", the company rose to account for half of the world\\'s trade during the mid-1700s and early 1800s, particularly in basic commodities including cotton, silk, indigo dye, sugar, salt, spices, saltpetre, tea, and later, opium. The company also initiated the beginnings of the British Empire in South Asia.\\nThe company eventually came to rule large areas of present day Bangladesh, Pakistan and India, exercising military power and assuming administrative functions. Company-ruled areas in the region  gradually expanded after the Battle of Polashi (Plassey) in 1757 and by 1858 most of modern India, Pakistan and Bangladesh was either ruled by the company or princely states closely tied to it by treaty. Following the Sepoy Rebellion of 1857, the Government of India Act 1858 led to the British Crown assuming direct control of present day Bangladesh, Pakistan and India in the form of the new British Indian Empire.\\nThe company subsequently experienced recurring problems with its finances, despite frequent government intervention. The company was dissolved in 1874 under the terms of the East India Stock Dividend Redemption Act enacted one year earlier, as the Government of India Act had by then rendered it vestigial, powerless, and obsolete. The official government machinery of the British Empire had assumed its governmental functions and absorbed its armies.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/East_India_Company'}, page_content='The East India Company (E'),\n",
       " Document(metadata={'title': 'Economy of India', 'summary': \"The economy of India is a developing mixed economy with a notable public sector in strategic sectors. It is the world's fifth-largest economy by nominal GDP and the third-largest by purchasing power parity (PPP); on a per capita income basis, India ranked 136th by GDP (nominal) and 125th by GDP (PPP). From independence in 1947 until 1991, successive governments followed the Soviet model and promoted protectionist economic policies, with extensive Sovietization, state intervention, demand-side economics, natural resources, bureaucrat driven enterprises and economic regulation. This is characterised as dirigism, in the form of the Licence Raj. The end of the Cold War and an acute balance of payments crisis in 1991 led to the adoption of a broad economic liberalisation in India and indicative planning. Since the start of the 21st century, annual average GDP growth has been 6% to 7%., India has about 1,900 public sector companies, Indian state has complete control and ownership of railways, highways; majority control and stake in banking, insurance, farming, dairy, fertilizers & chemicals, airports, nuclear, mining, digitization, defense, steel, rare earths, water, electricity, oil and gas industries and power plants, and has substantial control over digitalization, Broadband as national infrastructure, telecommunication, supercomputing, space, port and shipping industries, among other industries, were effectively nationalised in the mid-1950s.\\nNearly 70% of India's GDP is driven by domestic consumption; country remains the world's fourth-largest consumer market. Apart from private consumption, India's GDP is also fueled by government spending, investments, and exports. In 2022, India was the world's 10th-largest importer and the 8th-largest exporter. India has been a member of the World Trade Organization since 1 January 1995. It ranks 63rd on the Ease of doing business index and 40th on the Global Competitiveness Index. India has one of the world's highest number of billionaires and extreme income inequality. Economists and social scientists often consider India a welfare state. India is officially declared a socialist state as per the constitution. India's overall social welfare spending stood at 8.6% of GDP in 2021-22, which is much lower than the average for OECD nations. With 586 million workers, the Indian labour force is the world's second-largest.\\nDuring the Great Recession, the economy faced a mild slowdown. India endorsed Keynesian policy and initiated stimulus measures (both fiscal and monetary) to boost growth and generate demand. In subsequent years, economic growth revived.\\nIn 2021–22, the foreign direct investment (FDI) in India was $82 billion. The leading sectors for FDI inflows were the Finance, Banking, Insurance and R&D. India has free trade agreements with several nations and blocs, including ASEAN, SAFTA, Mercosur, South Korea, Japan, Australia, UAE, and several others which are in effect or under negotiating stage. In recent years, independent economists and financial institutions have accused the government of manipulating various economic data, especially GDP growth rate.\\nThe service sector makes up  more than 50% of GDP and remains the fastest growing sector, while the industrial sector and the agricultural sector employs a majority of the labor force. The Bombay Stock Exchange and National Stock Exchange are some of the world's largest stock exchanges by market capitalisation. India is the world's sixth-largest manufacturer, representing 2.6% of global manufacturing output. Nearly 65% of India's population is rural, and contributes about 50% of India's GDP. India faces high unemployment, rising income inequality, and a drop in aggregate demand. India's gross domestic savings rate stood at 29.3% of GDP in 2022.\", 'source': 'https://en.wikipedia.org/wiki/Economy_of_India'}, page_content='The economy of India is a'),\n",
       " Document(metadata={'title': 'History of India', 'summary': \"Anatomically modern humans first arrived on the Indian subcontinent between 73,000 and 55,000 years ago. The earliest known human remains in South Asia date to 30,000 years ago. Sedentariness began in South Asia around 7000 BCE; by 4500 BCE, settled life had spread, and gradually evolved into the Indus Valley Civilisation, which flourished between 2500 BCE and 1900 BCE in present-day Pakistan and north-western India. Early in the second millennium BCE, persistent drought caused the population of the Indus Valley to scatter from large urban centres to villages. Indo-Aryan tribes moved into the Punjab from Central Asia in several waves of migration. The Vedic Period of the Vedic people in northern India (1500–500 BCE) was marked by the composition of their extensive collections of hymns (Vedas). The social structure was loosely stratified via the varna system, incorporated into the highly evolved present-day Jāti system. The pastoral and nomadic Indo-Aryans spread from the Punjab into the Gangetic plain. Around 600 BCE, a new, interregional culture arose; then, small chieftaincies (janapadas) were consolidated into larger states (mahajanapadas). Second urbanization took place, which came with the rise of new ascetic movements and religious concepts, including the rise of Jainism and Buddhism. The latter was synthesized with the preexisting religious cultures of the subcontinent, giving rise to Hinduism.\\n\\nChandragupta Maurya overthrew the Nanda Empire and established the first great empire in ancient India, the Maurya Empire. India's Mauryan king Ashoka is widely recognised for his historical acceptance of Buddhism and his attempts to spread nonviolence and peace across his empire. The Maurya Empire would collapse in 185 BCE, on the assassination of the then-emperor Brihadratha by his general Pushyamitra Shunga. Shunga would form the Shunga Empire in the north and north-east of the subcontinent, while the Greco-Bactrian Kingdom would claim the north-west and found the Indo-Greek Kingdom. Various parts of India were ruled by numerous dynasties, including the Gupta Empire, in the 4th to 6th centuries CE. This period, witnessing a Hindu religious and intellectual resurgence is known as the Classical or Golden Age of India. Aspects of Indian civilisation, administration, culture, and religion spread to much of Asia, which led to the establishment of Indianised kingdoms in the region, forming Greater India. The most significant event between the 7th and 11th centuries was the Tripartite struggle centred on Kannauj. Southern India saw the rise of multiple imperial powers from the middle of the fifth century. The Chola dynasty conquered southern India in the 11th century. In the early medieval period, Indian mathematics, including Hindu numerals, influenced the development of mathematics and astronomy in the Arab world, including the creation of the Hindu-Arabic numeral system.\\nIslamic conquests made limited inroads into modern Afghanistan and Sindh as early as the 8th century, followed by the invasions of Mahmud Ghazni.\\nThe Delhi Sultanate was founded in 1206 by Central Asian Turks who were Indianized. They ruled a major part of the northern Indian subcontinent in the early 14th century. It was ruled by Multiple Turk, Afghan and Indian dynasties, Including the Turco-Mongol Indianized Tughlaq Dynasty but declined in the late 14th century following the invasions of Timur and saw the advent of the Malwa, Gujarat, and Bahmani Sultanates, the last of which split in 1518 into the five Deccan sultanates. The wealthy Bengal Sultanate also emerged as a major power, lasting over three centuries. During this period, multiple strong Hindu kingdoms, notably the Vijayanagara Empire and the Rajput states, emerged and played significant roles in shaping the cultural and political landscape of India.\\nThe early modern period began in the 16th century, when the Mughal Empire conquered most of the Indian subcontinent, signaling the proto-industrialisation, becoming the biggest global economy and manufacturing power. The Mughals suffered a gradual decline in the early 18th century, largely due to the rising power of the Marathas, who took control of extensive regions of the Indian subcontinent. The East India Company, acting as a sovereign force on behalf of the British government, gradually acquired control of huge areas of India between the middle of the 18th and the middle of the 19th centuries. Policies of company rule in India led to the Indian Rebellion of 1857. India was afterwards ruled directly by the British Crown, in the British Raj. After World War I, a nationwide struggle for independence was launched by the Indian National Congress, led by Mahatma Gandhi. Later, the All-India Muslim League would advocate for a separate Muslim-majority nation state. The British Indian Empire was partitioned in August 1947 into the Dominion of India and Dominion of Pakistan, each gaining its independence.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/History_of_India'}, page_content='Anatomically modern human'),\n",
       " Document(metadata={'title': 'Government of India', 'summary': 'The Government of India (ISO: Bhārata Sarakāra, legally the Union Government or Union of India and colloquially known as the Central Government) is the central executive authority of the Republic of India, a federal republic located in South Asia, consisting of 28 states and eight union territories. The government is led by the prime minister (currently Narendra Modi since 26 May 2014) who exercises the most executive power and selects all the other ministers. The country has been governed by a NDA-led government (a coalition of the BJP and its allies) since 2014. The prime minister and their senior ministers belong to the Union Council of Ministers—its executive decision-making committee being the cabinet.\\nThe government, seated in New Delhi, has three primary branches: the legislative, the executive and the judiciary, whose powers are vested in a bicameral Parliament, a prime minister, and the Supreme Court respectively, with a president as head of state.\\nThe Council of Ministers are responsible to the House in which they sit, they make statements in that House and take questions from fellow members of that House. For most senior ministers this is usually the directly elected Lok Sabha rather than the (mostly) indirectly elected Rajya Sabha. As is the case in most parliamentary systems, the government is dependent on Parliament to legislate, and general elections are held every five years to elect a new Lok Sabha. The most recent election was in 2024.\\nAfter an election, the president selects as prime minister the leader of the party or alliance most likely to command the confidence of the majority of the Lok Sabha. In the event that the prime minister is not a member of either House upon appointment, he/she is given six months to be elected to either House of Parliament.', 'source': 'https://en.wikipedia.org/wiki/Government_of_India'}, page_content='The Government of India ('),\n",
       " Document(metadata={'title': 'India at the Paralympics', 'summary': \"India first participated in the 1968 Summer Paralympics. The nation has appeared in every edition of the Summer Paralympics since 1984. The Paris 2024 Games marked India's 13th appearance at the Paralympics. The country has never participated in the Winter Paralympic Games.\\nIndia's first medal in the Paralympics came in the 1972 Games, with Murlikant Petkar winning a gold medal in swimming. Up to the recent 2024 Games, India have won 60 medals across all Paralympic Games, with the most successful Paralympic campaign being the Paris 2024 Games with 29 medals including seven gold, nine silver and thirteen bronze.\", 'source': 'https://en.wikipedia.org/wiki/India_at_the_Paralympics'}, page_content='India first participated '),\n",
       " Document(metadata={'title': 'Constitution of India', 'summary': \"The Constitution of India is the supreme law of India. The document lays down the framework that demarcates fundamental political code, structure, procedures, powers, and duties of government institutions and sets out fundamental rights, directive principles, and the duties of citizens. It is the longest written national constitution in the world.\\nIt imparts constitutional supremacy (not parliamentary supremacy, since it was created by a constituent assembly rather than Parliament) and was adopted by its people with a declaration in its preamble. Parliament cannot override the constitution.\\n\\nIt was adopted by the Constituent Assembly of India on 26 November 1949 and became effective on 26 January 1950. The constitution replaced the Government of India Act 1935 as the country's fundamental governing document, and the Dominion of India became the Republic of India. To ensure constitutional autochthony, its framers repealed prior acts of the British parliament in Article 395. India celebrates its constitution on 26 January as Republic Day.\\nThe constitution declares India a sovereign, socialist, secular, and democratic republic, assures its citizens justice, equality, and liberty, and endeavours to promote fraternity. The original 1950 constitution is preserved in a nitrogen-filled case at the Old Parliament House in New Delhi.\", 'source': 'https://en.wikipedia.org/wiki/Constitution_of_India'}, page_content='The Constitution of India'),\n",
       " Document(metadata={'title': 'Languages of India', 'summary': 'Languages spoken in the Republic of India belong to several language families, the major ones being the Indo-Aryan languages spoken by 78.05% of Indians and the Dravidian languages spoken by 19.64% of Indians; both families together are sometimes known as Indic languages. Languages spoken by the remaining 2.31% of the population belong to the Austroasiatic, Sino–Tibetan, Tai–Kadai, and a few other minor language families and isolates.:\\u200a283\\u200a According to the People\\'s Linguistic Survey of India, India has the second highest number of languages (780), after Papua New Guinea (840). Ethnologue lists a lower number of 456.\\nArticle 343 of the Constitution of India stated that the official language of the Union is Hindi in Devanagari script, with official use of English to continue for 15 years from 1947. Later, a constitutional amendment, The Official Languages Act, 1963, allowed for the continuation of English alongside Hindi in the Indian government indefinitely until legislation decides to change it. The form of numerals to be used for the official purposes of the Union are \"the international form of Indian numerals\", which are referred to as Arabic numerals in most English-speaking countries. Despite some misconceptions, Hindi is not the national language of India; the Constitution of India does not give any language the status of national language.\\nThe Eighth Schedule of the Indian Constitution lists 22 languages, which have been referred to as scheduled languages and given recognition, status and official encouragement. In addition, the Government of India has awarded the distinction of classical language to Kannada, Malayalam, Odia, Sanskrit, Tamil and Telugu. This status is given to languages that have a rich heritage and independent nature.\\nAccording to the Census of India of 2001, India has 122 major languages and 1599 other languages. However, figures from other sources vary, primarily due to differences in the definition of the terms \"language\" and \"dialect\". The 2001 Census recorded 30 languages which were spoken by more than a million native speakers and 122 which were spoken by more than 10,000 people. Two contact languages have played an important role in the history of India: Persian and English. Persian was the court language during the Mughal period in India and reigned as an administrative language for several centuries until the era of British colonisation. English continues to be an important language in India. It is used in higher education and in some areas of the Indian government.\\nHindi, which has the largest number of first-language speakers in India today, serves as the lingua franca across much of northern and central India. However, there have been concerns raised with Hindi being imposed in South India, most notably in the states of Tamil Nadu and Karnataka. Some in Maharashtra, West Bengal, Assam, Punjab and other non-Hindi regions have also started to voice concerns about imposition of Hindi. Bengali is the second most spoken and understood language in the country with a significant number of speakers in eastern and northeastern regions. Marathi is the third most spoken and understood language in the country with a significant number of speakers in the southwest, followed closely by Telugu, which is most commonly spoken in southeastern areas.\\nHindi is the fastest growing language of India, followed by Kashmiri in the second place, with Meitei (officially called Manipuri) as well as Gujarati, in the third place, and Bengali in the fourth place, according to the 2011 census of India.\\nAccording to the Ethnologue, India has 148 Sino-Tibetan, 140 Indo-European, 84 Dravidian, 32 Austro-Asiatic, 14 Andamanese, 5 Kra-Dai languages.', 'source': 'https://en.wikipedia.org/wiki/Languages_of_India'}, page_content='Languages spoken in the R'),\n",
       " Document(metadata={'title': 'British Raj', 'summary': \"The British Raj ( RAHJ; from Hindustani rāj, 'reign', 'rule' or 'government') was the rule of the British Crown on the Indian subcontinent, lasting from 1858 to 1947. It is also called Crown rule in India,\\nor Direct rule in India. The region under British control was commonly called India in contemporaneous usage and included areas directly administered by the United Kingdom, which were collectively called British India, and areas ruled by indigenous rulers, but under British paramountcy, called the princely states. The region was sometimes called the Indian Empire, though not officially.\\nThis system of governance was instituted on 28 June 1858, when, after the Indian Rebellion of 1857, the rule of the East India Company was transferred to the Crown in the person of Queen Victoria (who, in 1876, was proclaimed Empress of India). It lasted until 1947, when the British Raj was partitioned into two sovereign dominion states: the Union of India (later the Republic of India) and Pakistan (later the Islamic Republic of Pakistan). Later, the People's Republic of Bangladesh gained independence from Pakistan. At the inception of the Raj in 1858, Lower Burma was already a part of British India; Upper Burma was added in 1886, and the resulting union, Burma, was administered as an autonomous province until 1937, when it became a separate British colony, gaining its own independence in 1948. It was renamed Myanmar in 1989. The Chief Commissioner's Province of Aden was also part of British India at the inception of the British Raj, and became a separate colony known as Aden Colony in 1937 as well.\\nAs India, it was a founding member of the League of Nations, and a founding member of the United Nations in San Francisco in 1945. India was a participating state in the Summer Olympics in 1900, 1920, 1928, 1932, and 1936.\", 'source': 'https://en.wikipedia.org/wiki/British_Raj'}, page_content='The British Raj ( RAHJ; f'),\n",
       " Document(metadata={'title': 'Demographics of India', 'summary': \"India is the most populous country in the world with one-sixth of the world's population. \\nAccording to estimates from the United Nations (UN), India has overtaken China as the country with the largest population in the world, with a population of 1,425,775,850 at the end of April 2023.\\nBetween 1975 and 2010, the population doubled to 1.2 billion, reaching the billion mark in 2000. According to the UN's World Population dashboard, India's population now stands at slightly over 1.428 billion, edging past China's population of 1.425 billion people, as reported by the news agency Bloomberg. In 2015, India's population was predicted to reach 1.7 billion by 2050. In 2017 its population growth rate was 0.98%, ranking 112th in the world; in contrast, from 1972 to 1983, India's population grew by an annual rate of 2.3%.\\nIn 2023, the median age of an Indian was 29.5 years, compared to 39.8 for China and 49.5 for Japan; and, by 2030; India's dependency ratio will be just over 0.4. However, the number of children in India peaked more than a decade ago and is now falling. The number of children under the age of five peaked in 2007, and since then the number has been falling. The number of Indians under 15 years old peaked slightly later (in 2011) and is now also declining.\\nIndia has many ethnic groups, and every major religion is represented, as are four major families of languages (Indo-European, Dravidian, Austroasiatic and Sino-Tibetan languages) as well as two language isolates: the Nihali language, spoken in parts of Maharashtra, and the Burushaski language, spoken in parts of Jammu and Kashmir. 1,000,000 people in India are Anglo-Indians and 700,000 United States citizens are living in India. They represent over 0.1% of the total population of India. Overall, only the continent of Africa exceeds the linguistic, genetic and cultural diversity of the nation of India.\\nThe sex ratio was 944 females for 1000 males in 2016, and 940 per 1000 in 2011. This ratio has been showing an upwards trend for the last two decades after a continuous decline in the 20th century.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Demographics_of_India'}, page_content='India is the most populou'),\n",
       " Document(metadata={'title': 'India, India', 'summary': '\"India, India\" is a song written by John Lennon and recorded in 1980, but not released until 2010\\'s John Lennon Signature Box. The song, with \"I Don\\'t Want to Lose You\", was also featured in the 2005 musical Lennon.', 'source': 'https://en.wikipedia.org/wiki/India,_India'}, page_content='\"India, India\" is a song '),\n",
       " Document(metadata={'title': 'List of airlines of India', 'summary': 'This is a list of airlines currently operating in India.', 'source': 'https://en.wikipedia.org/wiki/List_of_airlines_of_India'}, page_content='This is a list of airline'),\n",
       " Document(metadata={'title': 'Partition of India', 'summary': \"The Partition of India in 1947 was the change of political borders and the division of other assets that accompanied the dissolution of the British Raj in the Indian subcontinent and the creation of two independent dominions in South Asia: India and Pakistan. The Dominion of India is today the Republic of India, and the Dominion of Pakistan—which at the time comprised two regions lying on either side of India—is now the Islamic Republic of Pakistan and the People's Republic of Bangladesh. The partition was outlined in the Indian Independence Act 1947. The change of political borders notably included the division of two provinces of British India, Bengal and Punjab. The majority Muslim districts in these provinces were awarded to Pakistan and the majority non-Muslim to India. The other assets that were divided included the British Indian Army, the Royal Indian Navy, the Royal Indian Air Force, the Indian Civil Service, the railways, and the central treasury. Provisions for self-governing independent Pakistan and India legally came into existence at midnight on 14 and 15 August 1947 respectively.\\nThe partition caused large-scale loss of life and an unprecedented migration between the two dominions. Among refugees who survived, it solidified the belief that safety lay among co-religionists. In the instance of Pakistan, it made palpable a hitherto only-imagined refuge for the Muslims of British India. The migrations took place hastily and with little warning. It is thought that between 14 million and 18 million people moved, and perhaps more. Excess mortality during the period of the partition is usually estimated to have been around one million. On 13 January 1948, Mahatma Gandhi started his fast with the goal of stopping the violence. He ended his fast on 18 January at the urging of various religious and political leaders, who pledged to put an end to the violence and uphold communal harmony.\\nThe term partition of India does not cover:\\n\\nthe separation of Burma (present day Myanmar) from the British Raj in 1937\\nthe much earlier separation of Ceylon (present day Sri Lanka) from the rule of the East India Company in 1796.\\nOther political entities or transformations in the region that were not a part of the partition were:\\nthe political integration of princely states into the two new dominions;\\nthe annexation of the princely states of Hyderabad and Junagadh by India;\\nthe dispute and division of the princely state of Jammu and Kashmir between India, Pakistan, and later China;\\nthe incorporation of the enclaves of French India into India during the period 1947–1954;\\nthe annexation of Goa and other districts of Portuguese India by India in 1961;\\nthe secession of Bangladesh from Pakistan in 1971.\\nNepal and Bhutan signed treaties with the British designating them as independent states and were not a part of British-ruled India. The Himalayan Kingdom of Sikkim was established as a princely state after the Anglo-Sikkimese Treaty of 1861, but its sovereignty had been left undefined. In 1947, Sikkim became an independent kingdom under the suzerainty of India. The Maldives became a protectorate of the British crown in 1887 and gained its independence in 1965.\", 'source': 'https://en.wikipedia.org/wiki/Partition_of_India'}, page_content='The Partition of India in'),\n",
       " Document(metadata={'title': 'Kerala', 'summary': \"Kerala (English:  / KERR-ə-lə), called Keralam in Malayalam (Malayalam: [keːɾɐɭɐm] ), is a state on the Malabar Coast of India. It was formed on 1 November 1956, following the passage of the States Reorganisation Act, by combining Malayalam-speaking regions of the erstwhile regions of Cochin, Malabar, South Canara, and Travancore. Spread over 38,863 km2 (15,005 sq mi), Kerala is the 21st largest Indian state by area. It is bordered by Karnataka to the north and northeast, Tamil Nadu to the east and south, and the Lakshadweep Sea to the west. With 33 million inhabitants as per the 2011 census, Kerala is the 13th-largest Indian state by population. It is divided into 14 districts with the capital being Thiruvananthapuram. Malayalam is the most widely spoken language and is also the official language of the state.\\nThe Chera dynasty was the first prominent kingdom based in Kerala. The Ay kingdom in the deep south and the Ezhimala kingdom in the north formed the other kingdoms in the early years of the Common Era (CE). The region had been a prominent spice exporter since 3000 BCE. The region's prominence in trade was noted in the works of Pliny as well as the Periplus around 100 CE. In the 15th century, the spice trade attracted Portuguese traders to Kerala, and paved the way for European colonisation of India. At the time of Indian independence movement in the early 20th century, there were two major princely states in Kerala: Travancore and Cochin. They united to form the state of Thiru-Kochi in 1949. The Malabar region, in the northern part of Kerala, had been a part of the Madras province of British India, which later became a part of the Madras State post-independence. After the States Reorganisation Act, 1956, the modern-day state of Kerala was formed by merging the Malabar district of Madras State (excluding Gudalur taluk of Nilgiris district, Lakshadweep Islands, Topslip, the Attappadi Forest east of Anakatti), the taluk of Kasaragod (now Kasaragod District) in South Canara, and the erstwhile state of Thiru-Kochi (excluding four southern taluks of Kanyakumari district, and Shenkottai taluks).\\nKerala has the lowest positive population growth rate in India, 3.44%; the highest Human Development Index (HDI), 0.784 in 2018 (0.712 in 2015); the highest literacy rate, 96.2% in the 2018 literacy survey conducted by the National Statistical Office, India; the highest life expectancy, 77.3 years; and the highest sex ratio, 1,084 women per 1,000 men. Kerala is the least impoverished state in India according to NITI Aayog's Sustainable Development Goals dashboard and Reserve Bank of India's 'Handbook of Statistics on Indian Economy'. Kerala is the second-most urbanised major state in the country with 47.7% urban population according to the 2011 Census of India. The state topped in the country to achieve the Sustainable Development Goals according to the annual report of NITI Aayog published in 2019. The state has the highest media exposure in India with newspapers publishing in nine languages, mainly Malayalam and sometimes English. Hinduism is practised by more than half of the population, followed by Islam and Christianity.\\nIn 2019–20, the economy of Kerala was the 8th-largest in India with ₹8.55 trillion (US$100 billion) in gross state domestic product (GSDP) and a per capita net state domestic product of ₹222,000 (US$2,700). \\nIn 2019–20, the tertiary sector contributed around 65% to state's GSVA, while the primary sector contributed only 8%. The state has witnessed significant emigration, especially to the Arab states of the Persian Gulf during the Gulf Boom of the 1970s and early 1980s, and its economy depends significantly on remittances from a large Malayali expatriate community. The production of pepper and natural rubber contributes significantly to the total national output. In the agricultural sector, coconut, tea, coffee, cashew and spices are important. The state is situated between Arabian Sea to the west and Western Ghats mountain ranges to the east. The state's coastline extends for 595 kilometres (370 mi), and around 1.1 million people in the state are dependent on the fishery industry, which contributes 3% to the state's income. Named as one of the ten paradises of the world by National Geographic Traveler, Kerala is one of the prominent tourist destinations of India, with coconut-lined sandy beaches, backwaters, hill stations, Ayurvedic tourism and tropical greenery as its major attractions.\", 'source': 'https://en.wikipedia.org/wiki/Kerala'}, page_content='Kerala (English:  / KERR-'),\n",
       " Document(metadata={'title': 'South India', 'summary': \"South India, also known as Southern India or Peninsular India, is the southern part of the Deccan Peninsula in India encompassing the states of Andhra Pradesh, Karnataka, Kerala, Tamil Nadu and Telangana as well as the union territories of Lakshadweep and Puducherry, occupying 19.31% of India's area (635,780 km2 or 245,480 sq mi) and 20% of India's population. It is bound by the Bay of Bengal in the east, the Arabian Sea in the west and the Indian Ocean in the south. The geography of the region is diverse, with two mountain ranges, the Western and Eastern Ghats, bordering the plateau heartland. The Godavari, Krishna, Kaveri, Tungabhadra and Vaigai rivers are important non-perennial sources of water. Chennai, Bangalore, Hyderabad, Coimbatore and Kochi are the largest urban areas in the region.\\nThe majority of the people in South India speak at least one of the four major Dravidian languages: Telugu, Tamil, Kannada and Malayalam. During its history, a number of dynastic kingdoms ruled over parts of South India, and shaped the culture in those regions. Major dynasties that were established in South India include the Cheras, Cholas, Pandyas, Pallavas, Satavahanas, Chalukyas, Hoysalas, Rashtrakutas and Vijayanagara. European countries entered India through Kerala and the region was colonized by Britain, Portugal and France.\\nAfter experiencing fluctuations in the decades immediately after Indian independence, the economies of South Indian states have registered a sustained higher-than-national-average growth over the past three decades. South India has the largest combined largest gross domestic product compared to other regions in India. The South Indian states lead in some socio-economic metrics of India with a higher HDI as the economy has undergone growth at a faster rate than in most northern states. As of 2011, Literacy rates in the southern states is higher than the national average at approximately 76%. The fertility rate in South India is 1.9, the lowest of all regions in India.\", 'source': 'https://en.wikipedia.org/wiki/South_India'}, page_content='South India, also known a'),\n",
       " Document(metadata={'title': 'States and union territories of India', 'summary': 'India is a federal union comprising 28 states and 8 union territories, for a total of 36 entities. The states and union territories are further subdivided into 806 districts and smaller administrative divisions.\\n\\nThe states of India are self-governing administrative divisions, each having a state government. The governing powers of the states are shared between the state government and the union government. On the other hand, the union territories are directly governed by the union government.', 'source': 'https://en.wikipedia.org/wiki/States_and_union_territories_of_India'}, page_content='India is a federal union '),\n",
       " Document(metadata={'title': 'Prime Minister of India', 'summary': \"The prime minister of India (ISO: Bhārata kē/kī pradhānamaṁtrī) is the head of government of the Republic of India. Executive authority is vested in the prime minister and his chosen Council of Ministers, despite the president of India being the nominal head of the executive.  The prime minister has to be a member of one of the houses of bicameral Parliament of India, alongside heading the respective house. The prime minister and his cabinet are at all times responsible to the Lok Sabha.\\nThe prime minister is appointed by the president of India; however, the prime minister has to enjoy the confidence of the majority of Lok Sabha members, who are directly elected every five years, lest the prime minister shall resign. The prime minister can be a member of the Lok Sabha or the Rajya Sabha, the upper house of the parliament. The prime minister controls the selection and dismissal of members of the Union Council of Ministers; and allocation of posts to members within the government.\\nThe longest-serving prime minister was Jawaharlal Nehru, also the first prime minister, whose tenure lasted 16 years and 286 days. His premiership was followed by Lal Bahadur Shastri's short tenure and Indira Gandhi's 11- and 4-year-long tenures, both politicians belonging to the Indian National Congress. After Indira Gandhi's assassination, her son Rajiv Gandhi took charge until 1989, when a decade with five unstable governments began. This was followed by the full terms of P. V. Narasimha Rao, Atal Bihari Vajpayee, Manmohan Singh, and Narendra Modi. Modi is the 14th and current prime minister of India, serving since 26 May 2014.\", 'source': 'https://en.wikipedia.org/wiki/Prime_Minister_of_India'}, page_content='The prime minister of Ind'),\n",
       " Document(metadata={'title': 'Índia', 'summary': 'Índia is the fourth studio album by Brazilian singer Gal Costa, released on 1973 by Philips Records. Its major hits were \"Índia\", \"Volta\" and \"Desafinado\".', 'source': 'https://en.wikipedia.org/wiki/%C3%8Dndia'}, page_content='Índia is the fourth studi'),\n",
       " Document(metadata={'title': 'Islam in India', 'summary': \"Islam is India's second-largest religion, with 14.2% of the country's population, or approximately 172.2 million people, identifying as adherents of Islam in a 2011 census. India also has the third-largest number of Muslims in the world. The majority of India's Muslims are Sunni, with Shia making up around 15% of the Muslim population.\\nIslam spread in Indian communities along the Arab coastal trade routes in Gujarat and along the Malabar Coast shortly after the religion emerged in the Arabian Peninsula. Islam arrived in the inland of Indian subcontinent in the 7th century when the Arabs conquered Sindh and later arrived in Punjab and North India in the 12th century via the Ghaznavids and Ghurids conquest and has since become a part of India's religious and cultural heritage. The Barwada Mosque in Ghogha, Gujarat built before 623 CE, Cheraman Juma Mosque (629 CE) in Methala, Kerala and Palaiya Jumma Palli (or The Old Jumma Masjid, 628–630 CE) in Kilakarai, Tamil Nadu are three of the first mosques in India which were built by seafaring Arab merchants. According to the legend of Cheraman Perumals, the first Indian mosque was built in 624 CE at Kodungallur in present-day Kerala with the mandate of the last ruler (the Tajudeen Cheraman Perumal) of the Chera dynasty, who converted to Islam during the lifetime of Muhammad (c. 570–632). Similarly, Tamil Muslims on the eastern coasts also claim that they converted to Islam in Muhammad's lifetime. The local mosques date to the early 700s.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Islam_in_India'}, page_content=\"Islam is India's second-l\"),\n",
       " Document(metadata={'title': 'List of cities in India by population', 'summary': 'This is a list of the most populous cities in India. Cities are a type of sub-administrative unit and are defined by the Ministry of Home Affairs. In some cases, cities are bifurcated into municipalities, which can lead to cities being included within other cities. This list is based on the Census of India using data from the 2001 census of India and the 2011 census of India.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/List_of_cities_in_India_by_population'}, page_content='This is a list of the mos'),\n",
       " Document(metadata={'title': 'President of India', 'summary': \"The president of India  (ISO: Bhārata kē/kī Rāṣṭrapati) is the head of state of the Republic of India. The president is the nominal head of the executive, the first citizen of the country, as well as the supreme commander of the Indian Armed Forces. Droupadi Murmu is the 15th and current president, having taken office from 25 July 2022.\\nThe office of president was created when India became a republic on 26 January 1950 when its constitution came into force. The president is indirectly elected by an electoral college comprising both houses of the Parliament of India and the legislative assemblies of each of India's states and territories, who themselves are all directly elected by the citizens.\\nArticle 53 of the Constitution of India states that the president can exercise their powers directly or by subordinate authority, though all of the executive powers vested in the president are, in practice, exercised by the prime minister heading the Council of Ministers. The president is bound by the constitution to act on the advice of the council and to enforce the decrees passed by the Supreme Court under article 142.\", 'source': 'https://en.wikipedia.org/wiki/President_of_India'}, page_content='The president of India  ('),\n",
       " Document(metadata={'title': 'India at the 2024 Summer Paralympics', 'summary': \"India competed at the 2024 Summer Paralympics in Paris from 28 August to 8 September 2024. The nation made its official debut at the 1968 Summer Paralympics and has appeared in every edition of the Summer Paralympics since 1984. This is India's 13th appearance at the Summer Paralympics.\\nIndia sent a contingent consisting of 84 athletes competing across 12 sports in the Paralympic Games. Bhagyashree Jadhav and Sumit Antil were the flag bearers during the opening ceremony. Later, Preethi Pal and Harvinder Singh served as the flag bearers during the closing ceremony.\\nIndia won 29 medals including seven gold, nine silver, and thirteen bronze medals. This marked India's highest ever medal tally in Paralympic Games surpassing the tally of 19 medals won in the 2020 Games.\", 'source': 'https://en.wikipedia.org/wiki/India_at_the_2024_Summer_Paralympics'}, page_content='India competed at the 202'),\n",
       " Document(metadata={'title': 'The Times of India', 'summary': 'The Times of India, also known by its abbreviation TOI, is an Indian English-language daily newspaper and digital news media owned and managed by The Times Group. It is the fourth-largest newspaper in India by circulation and largest selling English-language daily in the world. It is the oldest English-language newspaper in India, and the second-oldest Indian newspaper still in circulation, with its first edition published in 1838. It is nicknamed as \"The Old Lady of Bori Bunder\", and is a \"newspaper of record\".\\nNear the beginning of the 20th century, Lord Curzon, the Viceroy of India, called TOI \"the leading paper in Asia\". In 1991, the BBC ranked TOI among the world\\'s six best newspapers.\\nIt is owned and published by Bennett, Coleman & Co. Ltd. (B.C.C.L.), which is owned by the Sahu Jain family. In the Brand Trust Report India study 2019, TOI  was rated as the most trusted English newspaper in India. In a 2021 survey, Reuters Institute rated TOI as the most trusted media news brand among English-speaking, online news users in India. In recent decades, the newspaper has been criticised for establishing in the Indian news industry the practice of accepting payments from persons and entities in exchange for positive coverage.', 'source': 'https://en.wikipedia.org/wiki/The_Times_of_India'}, page_content='The Times of India, also ')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Youtube  loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.youtube.YoutubeLoader at 0x1861e925400>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "loader = YoutubeLoader(video_id=\"EkdrxF5YL24\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'EkdrxF5YL24'}, page_content='this brings us to the magic address feature which can help you reduce RTO by improving the accuracy of addresses which help ensure higher successful delivery rates all you need to do is simply enter the address and click on fill magic address will automatically fill all the address Fields click on next')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript = loader.load()\n",
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the key points of the document:\n",
      "\n",
      "* **Magic Address feature reduces RTO (Return to Origin):** This feature helps improve delivery rates by automatically verifying and correcting addresses. \n",
      "* **Simple to use:**  Users simply enter the address, click \"Fill Magic Address\", and the tool will populate the address fields correctly. \n",
      "* **Next step:** After the address is filled, users can click \"Next\" to proceed. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLEAI_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You summaries the given document in 2 or 3 points.\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt|llm|parser\n",
    "\n",
    "print(chain.invoke({\"input\": transcript[0]}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
