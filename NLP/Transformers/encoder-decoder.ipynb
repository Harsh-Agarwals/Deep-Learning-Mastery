{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from datasets) (0.25.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\mercadosemi\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\mercadosemi\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mercadosemi\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mercadosemi\\anaconda3\\envs\\genai\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mercadosemi\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation', 'en', 'fr'],\n",
       "        num_rows: 179435\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation', 'en', 'fr'],\n",
       "        num_rows: 903\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation', 'en', 'fr'],\n",
       "        num_rows: 3666\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"ahazeemi/iwslt14-en-fr\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'translation', 'en', 'fr'],\n",
       "     num_rows: 179435\n",
       " }),\n",
       " {'id': 'docid-1_segid-1',\n",
       "  'translation': {'en': 'It can be a very complicated thing, the ocean.',\n",
       "   'fr': \"Ca peut être très compliqué, l'océan.\"},\n",
       "  'en': 'It can be a very complicated thing, the ocean.',\n",
       "  'fr': \"Ca peut être très compliqué, l'océan.\"})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'], data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 2073, 2003, 26779, 1999, 1996, 2088, 4949, 1029, 102],\n",
       " [101, 2073, 2003, 2032, 7911, 3148, 3022, 1999, 1996, 2088, 4949, 1029, 102])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"Where is himalayas in the world map?\"\n",
    "sample2 = \"Where is himalayaas in the world map?\"\n",
    "encoding = tokenizer.encode(sample)\n",
    "encoding2 = tokenizer.encode(sample2)\n",
    "encoding, encoding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS]',\n",
       "  'where',\n",
       "  'is',\n",
       "  'himalayas',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'map',\n",
       "  '?',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'where',\n",
       "  'is',\n",
       "  'him',\n",
       "  '##ala',\n",
       "  '##ya',\n",
       "  '##as',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'map',\n",
       "  '?',\n",
       "  '[SEP]'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoding), tokenizer.convert_ids_to_tokens(encoding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2073, 2003, 26779, 4350, 1029, 102, 26779, 2024, 4350, 1999, 8222, 1010, 1998, 2003, 1996, 3284, 3137, 4672, 2006, 1996, 3011, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "q1 = \"Where is himalayas situated?\"\n",
    "a1 = \"Himalayas are situated in Nepal, and is the highest mountain peak on the earth.\"\n",
    "\n",
    "encoding = tokenizer.encode_plus(q1, a1)\n",
    "\n",
    "# input_ids: token considering join of q1 and a1\n",
    "# token_type_ids: 0: 1st sentence, 1: second sentence\n",
    "# attention_mask: attention with masking(padding)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'where',\n",
       " 'is',\n",
       " 'himalayas',\n",
       " 'situated',\n",
       " '?',\n",
       " '[SEP]',\n",
       " 'himalayas',\n",
       " 'are',\n",
       " 'situated',\n",
       " 'in',\n",
       " 'nepal',\n",
       " ',',\n",
       " 'and',\n",
       " 'is',\n",
       " 'the',\n",
       " 'highest',\n",
       " 'mountain',\n",
       " 'peak',\n",
       " 'on',\n",
       " 'the',\n",
       " 'earth',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 100, 100, 102, 100, 100, 102], 'token_type_ids': [0, 0, 0, 0, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2 = \"How is himalayas?\"\n",
    "a2 = \"Mysterious and white.\"\n",
    "\n",
    "encoding = tokenizer.encode_plus([q1, q2], [a1, a2])\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2073, 2003, 26779, 4350, 1029, 102, 26779, 2024, 4350, 1999, 8222, 1010, 1998, 2003, 1996, 3284, 3137, 4672, 2006, 1996, 3011, 1012, 102], [101, 2129, 2003, 26779, 1029, 102, 8075, 1998, 2317, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([q1, q2], [a1, a2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2073, 2003, 26779, 4350, 1029, 102, 26779, 2024, 4350, 1999, 8222, 1010, 1998, 2003, 1996, 3284, 3137, 4672, 2006, 1996, 3011, 1012, 102], [101, 2129, 2003, 26779, 1029, 102, 8075, 1998, 2317, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding padding (Adds 0 to the right)\n",
    "# Model will only focus where attention mask is 1\n",
    "\n",
    "tokenizer([q1, q2], [a1, a2], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2073, 2003, 26779, 4350, 1029, 102], [101, 2129, 2003, 26779, 1029, 102, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0]]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.batch_encode_plus([q1, q2], [a1, a2], padding=True)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'how', 'is', 'himalayas', '?', '[SEP]', '[PAD]']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]), \n",
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2073, 2003, 26779, 4350, 1029, 102], [101, 2129, 2003, 26779, 1029, 102, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0]]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distilbert tokenizer\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "tokenizer.batch_encode_plus([q1, q2], [a1, a2], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'where', 'is', 'himalayas', 'situated', '?', '[SEP]']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7c6b09e65c47ed969773e042d78474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/179435 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96425745393a4b28aa4d111276afed01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/903 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d61dc7714ee48378a30f50ccd78d8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3666 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def simple_tokenizer(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_data(batch):\n",
    "    # return_tensors=\"pt\": returns pytorch tensors\n",
    "    # padding=True: adds padding to the attention mask\n",
    "    # truncation=True: truncate to a maximum length specified by the max_length argument or the maximum length accepted by the model if no max_length is provided\n",
    "\n",
    "    # Tokenized source\n",
    "    tokenized_src = tokenizer(batch['en'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Tokenized target\n",
    "    tokenized_tgt = [simple_tokenizer(sentence) for sentence in batch['fr']]\n",
    "\n",
    "    return {\n",
    "        'input_ids': tokenized_src['input_ids'],\n",
    "        'attention_mask': tokenized_src['attention_mask'],\n",
    "        'labels': tokenized_tgt\n",
    "    }\n",
    "\n",
    "tokenized_data = data.map(tokenize_data, batched=True, remove_columns=['en', 'fr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['id', 'translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 179435\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['id', 'translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 903\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['id', 'translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 3666\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['id', 'translation', 'en', 'fr'],\n",
       "         num_rows: 179435\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['id', 'translation', 'en', 'fr'],\n",
       "         num_rows: 903\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['id', 'translation', 'en', 'fr'],\n",
       "         num_rows: 3666\n",
       "     })\n",
       " }))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'id': 'docid-1_segid-1',\n",
       "  'translation': {'en': 'It can be a very complicated thing, the ocean.',\n",
       "   'fr': \"Ca peut être très compliqué, l'océan.\"},\n",
       "  'input_ids': [101,\n",
       "   2009,\n",
       "   2064,\n",
       "   2022,\n",
       "   1037,\n",
       "   2200,\n",
       "   8552,\n",
       "   2518,\n",
       "   1010,\n",
       "   1996,\n",
       "   4153,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'labels': ['ca', 'peut', 'être', 'très', 'compliqué,', \"l'océan.\"]},\n",
       " {'id': 'docid-1_segid-1',\n",
       "  'translation': {'en': 'It can be a very complicated thing, the ocean.',\n",
       "   'fr': \"Ca peut être très compliqué, l'océan.\"},\n",
       "  'en': 'It can be a very complicated thing, the ocean.',\n",
       "  'fr': \"Ca peut être très compliqué, l'océan.\"})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['train'][0], data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'translation', 'input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['train'][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now the data is tokenized and we need to make the model.**\n",
    "\n",
    "# Defining models\n",
    "\n",
    "### Defining encoder\n",
    "### Defining decoder\n",
    "### Defining Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder = BertTokenizer.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers=1):\n",
    "        super(Decoder, self).__init__() # initialize nn.Module part of Decoder\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        embedded = self.embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
